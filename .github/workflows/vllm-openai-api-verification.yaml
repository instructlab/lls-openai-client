# spdx-license-identifier: Apache-2.0

name: vLLM OpenAI API Verification

on:
  workflow_dispatch:
    inputs:
      publish:
        description: "Publish test results?"
        required: true
        default: false
        type: boolean
  schedule:
    - cron: "21 01 * * *"

env:
  LC_ALL: en_US.UTF-8
  TMPDIR: /home/tmp
  VERIFICATION_RESULTS_DIR: /home/tmp/test_results
  VLLM_INFERENCE_MODEL: RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic

defaults:
  run:
    shell: bash

permissions:
  contents: read

jobs:
  start-ec2-runner:
    runs-on: ubuntu-latest
    outputs:
      label: ${{ steps.start-ec2-runner.outputs.label }}
      ec2-instance-id: ${{ steps.start-ec2-runner.outputs.ec2-instance-id }}
      ec2-instance-region: ${{ steps.start-ec2-runner.outputs.ec2-instance-region }}
    steps:
      - name: Checkout "launch-ec2-runner-with-fallback" in-house CI action
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          repository: instructlab/ci-actions
          # clone the "ci-actions" repo to a local directory called "ci-actions", instead of overwriting the current WORKDIR contents
          path: ci-actions
          ref: release-v0.1
          sparse-checkout: |
            actions/launch-ec2-runner-with-fallback

      - name: Launch EC2 Runner with Fallback
        id: start-ec2-runner
        uses: ./ci-actions/actions/launch-ec2-runner-with-fallback
        env:
          TMPDIR: "/tmp"
        with:
          aws_access_key_id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws_secret_access_key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          github_token: ${{ secrets.GH_PERSONAL_ACCESS_TOKEN }}
          regions_config: >
            [
              {
                "region": "us-east-2",
                "subnets": {
                  "us-east-2a": "${{ vars.SUBNET_US_EAST_2A }}",
                  "us-east-2b": "${{ vars.SUBNET_US_EAST_2B }}",
                  "us-east-2c": "${{ vars.SUBNET_US_EAST_2C }}"
                },
                "ec2-ami": "${{ vars.AWS_EC2_AMI_US_EAST_2 }}",
                "security-group-id": "${{ vars.SECURITY_GROUP_ID_US_EAST_2 }}"
              },
              {
                "region": "us-east-1",
                "subnets": {
                  "us-east-1a": "${{ vars.SUBNET_US_EAST_1A }}",
                  "us-east-1b": "${{ vars.SUBNET_US_EAST_1B }}",
                  "us-east-1c": "${{ vars.SUBNET_US_EAST_1C }}",
                  "us-east-1d": "${{ vars.SUBNET_US_EAST_1D }}",
                  "us-east-1e": "${{ vars.SUBNET_US_EAST_1E }}",
                  "us-east-1f": "${{ vars.SUBNET_US_EAST_1F }}"
                },
                "ec2-ami": "${{ vars.AWS_EC2_AMI_US_EAST_1 }}",
                "security-group-id": "${{ vars.SECURITY_GROUP_ID_US_EAST_1 }}"
              }
            ]
          try_spot_instance_first: false
          ec2_instance_type: g6e.12xlarge
          aws_resource_tags: >
            [
              {"Key": "Name", "Value": "instructlab-ci-github-large-runner"},
              {"Key": "GitHubRepository", "Value": "${{ github.repository }}"},
              {"Key": "GitHubRef", "Value": "${{ github.ref }}"}
            ]

  verification:
    needs:
      - start-ec2-runner
    runs-on: ${{ needs.start-ec2-runner.outputs.label }}

    steps:
      - name: Install Packages
        run: |
          cat /etc/os-release
          mkdir -p "${TMPDIR}"
          mkdir -p "${VERIFICATION_RESULTS_DIR}"
          sudo dnf install -y gcc gcc-c++ make git python3.11 python3.11-devel python3.11-pip

      - name: Checkout meta-llama/llama-stack
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          repository: "meta-llama/llama-stack"
          ref: "main"
          # https://github.com/actions/checkout/issues/249
          fetch-depth: 0

      - name: Install dependencies
        run: |
          export PATH="/home/ec2-user/.local/bin:/usr/local/cuda/bin:$PATH"
          nvidia-smi

          export UV_PYTHON_PREFERENCE=only-system
          python3.11 -m pip install uv
          uv sync --extra dev
          source .venv/bin/activate
          python3.11 -m ensurepip
          uv pip install -e .
          uv pip install pytest-json-report

          # newer huggingface-hub and vllm directly via pip (vs uv)
          python3.11 -m pip install "huggingface-hub>=0.30.0"
          python3.11 -m pip install "vllm>=0.8.4"

          uv run llama stack build --template remote-vllm --image-type venv

      - name: Run servers
        run: |
          source .venv/bin/activate

          # Download latest Llama 4 chat templates from vLLM
          curl https://raw.githubusercontent.com/vllm-project/vllm/d5615af9aee97ef44f46de722d48852eb5d40802/examples/tool_chat_template_llama4_json.jinja -o tool_chat_template_llama4_json.jinja
          curl https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/examples/tool_chat_template_llama4_pythonic.jinja -o tool_chat_template_llama4_pythonic.jinja

          # Start vLLM and wait for it to come up
          # TODO: this needs llama4_json tool call parser once that gets into a vllm release
          python3.11 -m vllm.entrypoints.openai.api_server \
            --model "${VLLM_INFERENCE_MODEL}" \
            --port 8000 \
            --enable-auto-tool-choice \
            --tool-call-parser pythonic \
            --chat-template tool_chat_template_llama4_pythonic.jinja \
            --gpu-memory-utilization 0.99 \
            --enforce-eager \
            --max-model-len 32000 \
            --tensor-parallel-size 4 2>&1 | tee vllm.log &
          curl --retry-connrefused --retry 50 --retry-delay 30 "${VLLM_URL}/models"

          # Start Llama Stack and wait for it to come up
          uv run llama stack run --image-type venv --port 8321 remote-vllm 2>&1 | tee llama_stack.log &
          curl --retry-connrefused --retry 5 --retry-delay 3 http://localhost:8321/v1/openai/v1/models
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          INFERENCE_MODEL: ${{ env.VLLM_INFERENCE_MODEL }}
          TAVILY_SEARCH_API_KEY: ${{ secrets.TAVILY_SEARCH_API_KEY }}
          VLLM_URL: "http://localhost:8000/v1"

      - name: Run vllm verification tests
        run: |
          source .venv/bin/activate

          cat <<-EOF > tests/verifications/conf/vllm.yaml
          base_url: http://localhost:8000/v1
          api_key_var: OPENAI_API_KEY
          models:
          - ${VLLM_INFERENCE_MODEL}
          model_display_names:
            ${VLLM_INFERENCE_MODEL}: ${VLLM_INFERENCE_MODEL}
          EOF

          cat <<-EOF > tests/verifications/conf/vllm-llama-stack.yaml
          base_url: http://localhost:8321/v1/openai/v1
          api_key_var: OPENAI_API_KEY
          models:
          - ${VLLM_INFERENCE_MODEL}
          model_display_names:
            ${VLLM_INFERENCE_MODEL}: ${VLLM_INFERENCE_MODEL}
          EOF

          tail -f vllm.log &
          tail -f llama_stack.log &

          # OpenAI Responses API
          python3.11 -m pytest -s -v tests/verifications/openai_api/test_responses.py --provider=vllm-llama-stack || echo "some vllm-llama-stack responses verification tests failed"

          # OpenAI Chat Completions API
          python3.11 -m pytest -s -v tests/verifications/openai_api/test_chat_completion.py --provider=vllm --json-report --json-report-file="${VERIFICATION_RESULTS_DIR}/vllm.json" || echo "some vllm verification tests failed"
          python3.11 -m pytest -s -v tests/verifications/openai_api/test_chat_completion.py --provider=vllm-llama-stack --json-report --json-report-file="${VERIFICATION_RESULTS_DIR}/vllm-llama-stack.json" || echo "some vllm-llama-stack verification tests failed"
        env:
          INFERENCE_MODEL: ${{ env.VLLM_INFERENCE_MODEL }}
          OPENAI_API_KEY: fake

      - name: Upload test artifacts
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        if: ${{ github.event_name == 'schedule' || inputs.publish }}
        with:
          name: openai-api-verification-results
          path: ${{ env.VERIFICATION_RESULTS_DIR }}

  publish-results:
    name: publish-results
    runs-on: ubuntu-latest
    if: ${{ github.event_name == 'schedule' || inputs.publish }}
    needs: verification
    permissions:
      # allow pushing to the github repo
      contents: write
    steps:
      - name: Checkout this repo
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          ref: "main"
          # https://github.com/actions/checkout/issues/249
          fetch-depth: 0

      - name: Download report artifact
        uses: actions/download-artifact@d3f86a106a0bac45b974a628896c90dbdf5c8093 # v4.3.0
        with:
          name: openai-api-verification-results
          path: artifacts

      - name: Add the new verification results
        run: |
          TODAY="$(date -Idate)"
          export TODAY
          ls -l artifacts/
          mkdir -p openai-api-verification/latest
          mkdir -p "openai-api-verification/${TODAY}"
          cp artifacts/*.json openai-api-verification/latest/
          cp artifacts/*.json "openai-api-verification/${TODAY}/"

      - name: Push new results
        run: |
          git config --global user.name "Ben Browning"
          git config --global user.email "bbrownin@redhat.com"

          git add openai-api-verification
          git commit -m "Verification results for $(date -Idate)"
          git push
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  stop-ec2-runner:
    needs:
      - start-ec2-runner
      - verification
      - publish-results
    runs-on: ubuntu-latest
    if: ${{ always() }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@b47578312673ae6fa5b5096b330d9fbac3d116df # v4.2.1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ needs.start-ec2-runner.outputs.ec2-instance-region }}

      - name: Stop EC2 runner
        uses: machulav/ec2-github-runner@fb91019e71385fb10dfcbec812b4de8c61589f7b # v2.4.1
        with:
          mode: stop
          github-token: ${{ secrets.GH_PERSONAL_ACCESS_TOKEN }}
          label: ${{ needs.start-ec2-runner.outputs.label }}
          ec2-instance-id: ${{ needs.start-ec2-runner.outputs.ec2-instance-id }}

  workflow-complete:
    # we don't want to block PRs on failed EC2 cleanup
    # so not requiring "stop-ec2-runner" as well
    needs: ["start-ec2-runner", "verification", "publish-results"]
    runs-on: ubuntu-latest
    steps:
      - name: Workflow Complete
        run: echo "Workflow Complete"
