{"created": 1745981789.18025, "duration": 94.35894680023193, "exitcode": 1, "root": "/actions-runner/_work/lls-openai-client/lls-openai-client", "environment": {}, "summary": {"passed": 24, "skipped": 4, "failed": 10, "total": 38, "collected": 38}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "earth"}, "setup": {"duration": 0.03438486999993984, "outcome": "passed"}, "call": {"duration": 4.279476974999852, "outcome": "passed"}, "teardown": {"duration": 0.0001794639999843639, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "saturn"}, "setup": {"duration": 0.007020271999863326, "outcome": "passed"}, "call": {"duration": 2.4923439609999605, "outcome": "passed"}, "teardown": {"duration": 0.00015072399992277496, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "earth"}, "setup": {"duration": 0.006930538999995406, "outcome": "passed"}, "call": {"duration": 0.43383542399988073, "outcome": "passed"}, "teardown": {"duration": 0.0001506430000972614, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "saturn"}, "setup": {"duration": 0.006700544000068476, "outcome": "passed"}, "call": {"duration": 2.640225796000095, "outcome": "passed"}, "teardown": {"duration": 0.00013590300000032585, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_missing"}, "setup": {"duration": 0.006265493999990213, "outcome": "passed"}, "call": {"duration": 0.00509272699991925, "outcome": "passed"}, "teardown": {"duration": 0.00022283499993136502, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006027428999914264, "outcome": "passed"}, "call": {"duration": 0.004719077999880028, "outcome": "passed"}, "teardown": {"duration": 0.00013566300003731158, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006005107999953907, "outcome": "passed"}, "call": {"duration": 0.13473094400001173, "outcome": "passed"}, "teardown": {"duration": 0.00014617300007557787, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.006735194999919258, "outcome": "passed"}, "call": {"duration": 0.07997091700008241, "outcome": "passed"}, "teardown": {"duration": 0.0001168930000403634, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.006599200999971799, "outcome": "passed"}, "call": {"duration": 0.09588698299990028, "outcome": "passed"}, "teardown": {"duration": 0.00011300200003461214, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_missing"}, "setup": {"duration": 0.006612322000137283, "outcome": "passed"}, "call": {"duration": 0.0029897389999860025, "outcome": "passed"}, "teardown": {"duration": 0.0001323429999047221, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006755264999810606, "outcome": "passed"}, "call": {"duration": 0.0036540639998747793, "outcome": "passed"}, "teardown": {"duration": 0.00010507199999665318, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006674712999938492, "outcome": "passed"}, "call": {"duration": 0.08599280399994313, "outcome": "passed"}, "teardown": {"duration": 0.00013204299989411084, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.006745435000084399, "outcome": "passed"}, "call": {"duration": 0.06461868399992454, "outcome": "passed"}, "teardown": {"duration": 0.00013359299987314444, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.006741424999972878, "outcome": "passed"}, "call": {"duration": 0.0846140439998635, "outcome": "passed"}, "teardown": {"duration": 0.00013380299992604705, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 133, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006743594999989, "outcome": "passed"}, "call": {"duration": 0.00011155199990753317, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 142, 'Skipped: Skipping test_chat_non_streaming_image for model llama3.3:70b-instruct-q4_K_M on provider ramalama-llama-stack based on config.')"}, "teardown": {"duration": 0.00010568299990154628, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 152, "outcome": "skipped", "keywords": ["test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.012925607000170203, "outcome": "passed"}, "call": {"duration": 0.00011092300019299728, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 161, 'Skipped: Skipping test_chat_streaming_image for model llama3.3:70b-instruct-q4_K_M on provider ramalama-llama-stack based on config.')"}, "teardown": {"duration": 0.00010498200003894453, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "calendar"}, "setup": {"duration": 0.006016408000050433, "outcome": "passed"}, "call": {"duration": 1.8202391040001658, "outcome": "passed"}, "teardown": {"duration": 0.00012187299989818712, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "math"}, "setup": {"duration": 0.006200097000146343, "outcome": "passed"}, "call": {"duration": 21.46502825099992, "outcome": "passed"}, "teardown": {"duration": 0.00014613299981647287, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "calendar"}, "setup": {"duration": 0.00645253000016055, "outcome": "passed"}, "call": {"duration": 1.8374286389998815, "outcome": "passed"}, "teardown": {"duration": 0.00012284400008866214, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "math"}, "setup": {"duration": 0.006207413999845812, "outcome": "passed"}, "call": {"duration": 15.067210439000064, "outcome": "passed"}, "teardown": {"duration": 0.0001382430000376189, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006256204000010257, "outcome": "passed"}, "call": {"duration": 1.816198446000044, "outcome": "passed"}, "teardown": {"duration": 0.0001283029998830898, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 245, "outcome": "failed", "keywords": ["test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006152401000008467, "outcome": "passed"}, "call": {"duration": 2.098204327000076, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 263, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fc0ac713d90>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=True,\n        )\n    \n>       _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:263: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fc0ac79e150>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00019290399995952612, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006363765999822135, "outcome": "passed"}, "call": {"duration": 1.6490781559998595, "outcome": "passed"}, "teardown": {"duration": 0.0001257530000202678, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006219352000016443, "outcome": "passed"}, "call": {"duration": 2.1636152269998092, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 316, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fc0ac6ee410>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n>       _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:316: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fc0ac6a6cd0>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00015511399988099583, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006431528000121034, "outcome": "passed"}, "call": {"duration": 1.7898395849999815, "outcome": "passed"}, "teardown": {"duration": 0.0001240439999037335, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 347, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006148784000060914, "outcome": "passed"}, "call": {"duration": 2.2832624299999225, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 367, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fc0ac6c3550>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n    \n        content = \"\"\n>       for chunk in stream:\n\ntests/verifications/openai_api/test_chat_completion.py:367: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fc0ac6d79d0>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.0001542439999866474, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.0064120399999865185, "outcome": "passed"}, "call": {"duration": 3.6393025119998583, "outcome": "passed"}, "teardown": {"duration": 0.00014223299990590021, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006333979000146428, "outcome": "passed"}, "call": {"duration": 2.7491664260001016, "outcome": "passed"}, "teardown": {"duration": 0.00014579399999092857, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "add_product_tool"}, "setup": {"duration": 0.006447089999937816, "outcome": "passed"}, "call": {"duration": 4.224219689999927, "outcome": "passed"}, "teardown": {"duration": 0.00013291299978845927, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006212453999978607, "outcome": "passed"}, "call": {"duration": 7.089208069999813, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 445, "message": "AssertionError: Expected arguments '{'name': 'Team Building', 'date': '2025-03-03', 'time': '10:00', 'location': 'Main Conference Room', 'participants': ['Alice', 'Bob', 'Charlie']}', got '{'date': '2025-03-03', 'time': '10:00', 'location': 'Main Conference Room', 'name': 'Team Building', 'participants': '[\"Alice\", \"Bob\", \"Charlie\"]'}'\nassert {'date': '202...arlie\"]', ...} == {'date': '202...harlie'], ...}\n  \n  Omitting 4 identical items, use -vv to show\n  Differing items:\n  {'participants': '[\"Alice\", \"Bob\", \"Charlie\"]'} != {'participants': ['Alice', 'Bob', 'Charlie']}\n  \n  Full diff:\n    {\n        'date': '2025-03-03',\n        'location': 'Main Conference Room',\n        'name': 'Team Building',\n  +     'participants': '[\"Alice\", \"Bob\", \"Charlie\"]',\n  -     'participants': [\n  -         'Alice',\n  -         'Bob',\n  -         'Charlie',\n  -     ],\n        'time': '10:00',\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 445, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fc0ac6f5d10>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'name': 'Team Building', 'date': '2025-03-03', 'time': '10:00', 'location': 'Main Conference Room', 'participants': ['Alice', 'Bob', 'Charlie']}', got '{'date': '2025-03-03', 'time': '10:00', 'location': 'Main Conference Room', 'name': 'Team Building', 'participants': '[\"Alice\", \"Bob\", \"Charlie\"]'}'\nE               assert {'date': '202...arlie\"]', ...} == {'date': '202...harlie'], ...}\nE                 \nE                 Omitting 4 identical items, use -vv to show\nE                 Differing items:\nE                 {'participants': '[\"Alice\", \"Bob\", \"Charlie\"]'} != {'participants': ['Alice', 'Bob', 'Charlie']}\nE                 \nE                 Full diff:\nE                   {\nE                       'date': '2025-03-03',\nE                       'location': 'Main Conference Room',\nE                       'name': 'Team Building',\nE                 +     'participants': '[\"Alice\", \"Bob\", \"Charlie\"]',\nE                 -     'participants': [\nE                 -         'Alice',\nE                 -         'Bob',\nE                 -         'Charlie',\nE                 -     ],\nE                       'time': '10:00',\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:445: AssertionError"}, "teardown": {"duration": 0.00013497300005838042, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006307055999968725, "outcome": "passed"}, "call": {"duration": 2.3937678920001417, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 445, "message": "AssertionError: Expected arguments '{'month': 1, 'year': 2025}', got '{'month': '1', 'year': '2025'}'\nassert {'month': '1', 'year': '2025'} == {'month': 1, 'year': 2025}\n  \n  Differing items:\n  {'month': '1'} != {'month': 1}\n  {'year': '2025'} != {'year': 2025}\n  \n  Full diff:\n    {\n  -     'month': 1,\n  +     'month': '1',\n  ?              + +\n  -     'year': 2025,\n  +     'year': '2025',\n  ?             +    +\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 445, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fc0ac6a6650>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'month': 1, 'year': 2025}', got '{'month': '1', 'year': '2025'}'\nE               assert {'month': '1', 'year': '2025'} == {'month': 1, 'year': 2025}\nE                 \nE                 Differing items:\nE                 {'month': '1'} != {'month': 1}\nE                 {'year': '2025'} != {'year': 2025}\nE                 \nE                 Full diff:\nE                   {\nE                 -     'month': 1,\nE                 +     'month': '1',\nE                 ?              + +\nE                 -     'year': 2025,\nE                 +     'year': '2025',\nE                 ?             +    +\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:445: AssertionError"}, "teardown": {"duration": 0.00012516300012066495, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006175602999974217, "outcome": "passed"}, "call": {"duration": 2.199556073000167, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fc0ac4319d0>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fc0ac6e7610>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00016362399992431165, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006282556999849476, "outcome": "passed"}, "call": {"duration": 2.1675115939999614, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7fc0ac705e50>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fc0ac6c3250>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00015357399979620823, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "add_product_tool"}, "setup": {"duration": 0.006400921000022208, "outcome": "passed"}, "call": {"duration": 2.2456550920001064, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fc0ac8fa510>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fc0ac6d7dd0>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.0001707240000996535, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006292627999982869, "outcome": "passed"}, "call": {"duration": 2.23683749099996, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fc0ac4fd690>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fc0ac692f10>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00016324400007761142, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006238366999923528, "outcome": "passed"}, "call": {"duration": 2.250853981999853, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "openai.APIError: 500: Internal server error: An unexpected error occurred."}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 501, "message": ""}, {"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 682, "message": "in _accumulate_streaming_tool_calls"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 72, "message": "APIError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fc0ac47f9d0>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n>           accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n\ntests/verifications/openai_api/test_chat_completion.py:501: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/verifications/openai_api/test_chat_completion.py:682: in _accumulate_streaming_tool_calls\n    for chunk in stream:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.Stream object at 0x7fc0ac44ebd0>\n\n    def __stream__(self) -> Iterator[_T]:\n        cast_to = cast(Any, self._cast_to)\n        response = self.response\n        process_data = self._client._process_response_data\n        iterator = self._iter_events()\n    \n        for sse in iterator:\n            if sse.data.startswith(\"[DONE]\"):\n                break\n    \n            if sse.event is None or sse.event.startswith(\"response.\") or sse.event.startswith('transcript.'):\n                data = sse.json()\n                if is_mapping(data) and data.get(\"error\"):\n                    message = None\n                    error = data.get(\"error\")\n                    if is_mapping(error):\n                        message = error.get(\"message\")\n                    if not message or not isinstance(message, str):\n                        message = \"An error occurred during streaming\"\n    \n>                   raise APIError(\n                        message=message,\n                        request=self.response.request,\n                        body=data[\"error\"],\n                    )\nE                   openai.APIError: 500: Internal server error: An unexpected error occurred.\n\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:72: APIError"}, "teardown": {"duration": 0.00015389400005005882, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "stream=False"}, "setup": {"duration": 0.006842592000111836, "outcome": "passed"}, "call": {"duration": 0.00011525299987624749, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama3.3:70b-instruct-q4_K_M on provider ramalama-llama-stack based on config.')"}, "teardown": {"duration": 0.00010649199998624681, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "stream=True"}, "setup": {"duration": 0.007367645999920569, "outcome": "passed"}, "call": {"duration": 9.734199988997716e-05, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama3.3:70b-instruct-q4_K_M on provider ramalama-llama-stack based on config.')"}, "teardown": {"duration": 0.0005737339999996038, "outcome": "passed"}}]}