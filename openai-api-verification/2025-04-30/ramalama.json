{"created": 1745981693.899491, "duration": 90.44822525978088, "exitcode": 1, "root": "/actions-runner/_work/lls-openai-client/lls-openai-client", "environment": {}, "summary": {"passed": 15, "failed": 19, "skipped": 4, "total": 38, "collected": 38}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "earth"}, "setup": {"duration": 0.07164361199988889, "outcome": "passed"}, "call": {"duration": 0.4407612810000501, "outcome": "passed"}, "teardown": {"duration": 0.00020926500019413652, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "saturn"}, "setup": {"duration": 0.007016646000010951, "outcome": "passed"}, "call": {"duration": 2.801153301999875, "outcome": "passed"}, "teardown": {"duration": 0.0001758640000844025, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "earth"}, "setup": {"duration": 0.006493782000006831, "outcome": "passed"}, "call": {"duration": 0.40773618599996553, "outcome": "passed"}, "teardown": {"duration": 0.00016183400020963745, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "saturn"}, "setup": {"duration": 0.006804411000075561, "outcome": "passed"}, "call": {"duration": 2.7966975690001163, "outcome": "passed"}, "teardown": {"duration": 0.00014435399998546927, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_missing"}, "setup": {"duration": 0.0069331479999164, "outcome": "passed"}, "call": {"duration": 1.4350366510000185, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "assert 400 == 500\n +  where 500 = InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}').status_code\n +    where InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}') = <ExceptionInfo InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is...}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}') tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]>>\nopenai_client = <openai.OpenAI object at 0x7fcc028585d0>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_missing', 'input': {'messages': []}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=False,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>       assert case[\"output\"][\"error\"][\"status_code\"] == e.value.status_code\nE       assert 400 == 500\nE        +  where 500 = InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}').status_code\nE        +    where InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}') = <ExceptionInfo InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is...}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}') tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:108: AssertionError"}, "teardown": {"duration": 0.00012919300002067757, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006201128999919092, "outcome": "passed"}, "call": {"duration": 3.3629484840000714, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fcc027b3610>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_role_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'fake_role'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.0001597839998339623, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006578379000075074, "outcome": "passed"}, "call": {"duration": 1.293220897000083, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "assert 400 == 500\n +  where 500 = InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\").status_code\n +    where InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\") tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fcc028cf450>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'invalid'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=False,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>       assert case[\"output\"][\"error\"][\"status_code\"] == e.value.status_code\nE       assert 400 == 500\nE        +  where 500 = InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\").status_code\nE        +    where InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\") tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:108: AssertionError"}, "teardown": {"duration": 0.00012051199996676587, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.006155128000045806, "outcome": "passed"}, "call": {"duration": 0.5333141270000397, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fcc026a0690>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.00013084300007903948, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.005862071000137803, "outcome": "passed"}, "call": {"duration": 1.4615826219999235, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "assert 400 == 500\n +  where 500 = InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \\'Failed to parse tools: Unsupported tool type: {\"type\":\"invalid\"}; tools = [\\\\n  {\\\\n    \"type\": \"invalid\"\\\\n  }\\\\n]\\', \\'type\\': \\'server_error\\'}}').status_code\n +    where InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \\'Failed to parse tools: Unsupported tool type: {\"type\":\"invalid\"}; tools = [\\\\n  {\\\\n    \"type\": \"invalid\"\\\\n  }\\\\n]\\', \\'type\\': \\'server_error\\'}}') = <ExceptionInfo InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \\'Failed to parse tools: Unsupported tool type: {\"type\":\"invalid\"}; tools = [\\\\n  {\\\\n    \"type\": \"invalid\"\\\\n  }\\\\n]\\', \\'type\\': \\'server_error\\'}}') tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fcc02991010>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=False,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>       assert case[\"output\"][\"error\"][\"status_code\"] == e.value.status_code\nE       assert 400 == 500\nE        +  where 500 = InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \\'Failed to parse tools: Unsupported tool type: {\"type\":\"invalid\"}; tools = [\\\\n  {\\\\n    \"type\": \"invalid\"\\\\n  }\\\\n]\\', \\'type\\': \\'server_error\\'}}').status_code\nE        +    where InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \\'Failed to parse tools: Unsupported tool type: {\"type\":\"invalid\"}; tools = [\\\\n  {\\\\n    \"type\": \"invalid\"\\\\n  }\\\\n]\\', \\'type\\': \\'server_error\\'}}') = <ExceptionInfo InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \\'Failed to parse tools: Unsupported tool type: {\"type\":\"invalid\"}; tools = [\\\\n  {\\\\n    \"type\": \"invalid\"\\\\n  }\\\\n]\\', \\'type\\': \\'server_error\\'}}') tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:108: AssertionError"}, "teardown": {"duration": 0.00014575299996977265, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_missing"}, "setup": {"duration": 0.006170408999878418, "outcome": "passed"}, "call": {"duration": 1.3760456700001669, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "assert '400' in 'Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}'\n +  where '400' = str(400)\n +  and   'Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}' = InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}').message\n +    where InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}') = <ExceptionInfo InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is...}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}') tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]>>\nopenai_client = <openai.OpenAI object at 0x7fcc026bc910>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_missing', 'input': {'messages': []}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n            for _chunk in response:\n                pass\n>       assert str(case[\"output\"][\"error\"][\"status_code\"]) in e.value.message\nE       assert '400' in 'Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}'\nE        +  where '400' = str(400)\nE        +  and   'Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}' = InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}').message\nE        +    where InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is 0) >= this->size() (which is 0) at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 7:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n      ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 40:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n                                       ^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 16, column 1:\\\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\\\n^\\\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}') = <ExceptionInfo InternalServerError('Error code: 500 - {\\'error\\': {\\'code\\': 500, \\'message\\': \"vector::_M_range_check: __n (which is...}\\\\n at row 1, column 1:\\\\n{{- bos_token }}\\\\n^\\\\n{%- if custom_tools is defined %}\\\\n\", \\'type\\': \\'server_error\\'}}') tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:131: AssertionError"}, "teardown": {"duration": 0.0001477139999224164, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006198779999976978, "outcome": "passed"}, "call": {"duration": 0.40799695199984853, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fcc02994f90>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_role_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'fake_role'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00011996300008831895, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.005948803000137559, "outcome": "passed"}, "call": {"duration": 1.2066612749999877, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "assert '400' in \"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\"\n +  where '400' = str(400)\n +  and   \"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\" = InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\").message\n +    where InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\") tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fcc0280d290>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'invalid'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n            for _chunk in response:\n                pass\n>       assert str(case[\"output\"][\"error\"][\"status_code\"]) in e.value.message\nE       assert '400' in \"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\"\nE        +  where '400' = str(400)\nE        +  and   \"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\" = InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\").message\nE        +    where InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Invalid tool_choice: invalid', 'type': 'server_error'}}\") tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:131: AssertionError"}, "teardown": {"duration": 0.00011784299999817449, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.005901343000004999, "outcome": "passed"}, "call": {"duration": 0.40705394900010106, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fcc0270dd10>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.0001451640000595944, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.005900632000020778, "outcome": "passed"}, "call": {"duration": 1.3587062680001054, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "assert '400' in \"Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\"\n +  where '400' = str(400)\n +  and   \"Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\" = InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\").message\n +    where InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\") tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fcc02809590>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n            for _chunk in response:\n                pass\n>       assert str(case[\"output\"][\"error\"][\"status_code\"]) in e.value.message\nE       assert '400' in \"Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\"\nE        +  where '400' = str(400)\nE        +  and   \"Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\" = InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\").message\nE        +    where InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\") = <ExceptionInfo InternalServerError(\"Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\") tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:131: AssertionError"}, "teardown": {"duration": 0.00014272399994297302, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 133, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006243710999797258, "outcome": "passed"}, "call": {"duration": 0.00011378299996067653, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 142, 'Skipped: Skipping test_chat_non_streaming_image for model llama3.3:70b-instruct-q4_K_M on provider ramalama based on config.')"}, "teardown": {"duration": 0.00010458199994900497, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 152, "outcome": "skipped", "keywords": ["test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006552977999945142, "outcome": "passed"}, "call": {"duration": 0.00010172199995395204, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 161, 'Skipped: Skipping test_chat_streaming_image for model llama3.3:70b-instruct-q4_K_M on provider ramalama based on config.')"}, "teardown": {"duration": 0.00010700200004976068, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "calendar"}, "setup": {"duration": 0.005983104999813804, "outcome": "passed"}, "call": {"duration": 1.889884234999954, "outcome": "passed"}, "teardown": {"duration": 0.00014581300001736963, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "math"}, "setup": {"duration": 0.006296792000057394, "outcome": "passed"}, "call": {"duration": 10.691785812000035, "outcome": "passed"}, "teardown": {"duration": 0.00017054399995686254, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "calendar"}, "setup": {"duration": 0.006419328999982099, "outcome": "passed"}, "call": {"duration": 1.7444164480000381, "outcome": "passed"}, "teardown": {"duration": 0.00014923399999133835, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "math"}, "setup": {"duration": 0.006546522000007826, "outcome": "passed"}, "call": {"duration": 16.912726307999947, "outcome": "passed"}, "teardown": {"duration": 0.0001445640000383719, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.010724148000008427, "outcome": "passed"}, "call": {"duration": 1.8063009700001658, "outcome": "passed"}, "teardown": {"duration": 0.00011967299997195369, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 245, "outcome": "failed", "keywords": ["test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006166877999930875, "outcome": "passed"}, "call": {"duration": 1.381838067000217, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 256, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fcc0254c710>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_calling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:256: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fcc0254c710>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\n\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00012826300007873215, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006031014999962281, "outcome": "passed"}, "call": {"duration": 1.642251232000035, "outcome": "passed"}, "teardown": {"duration": 0.0001387039999372064, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006110325000008743, "outcome": "passed"}, "call": {"duration": 1.416313910999861, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 308, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fcc027d6410>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:308: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fcc027d6410>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\n\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00012488299989854568, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006057573000134653, "outcome": "passed"}, "call": {"duration": 1.7803367409999282, "outcome": "passed"}, "teardown": {"duration": 0.00012312300009398314, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 347, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.005987520999951812, "outcome": "passed"}, "call": {"duration": 1.298125232999837, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 358, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fcc02952350>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n\ntests/verifications/openai_api/test_chat_completion.py:358: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fcc02952350>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\n\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00014462400008596887, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006240507000029538, "outcome": "passed"}, "call": {"duration": 3.620964938000043, "outcome": "passed"}, "teardown": {"duration": 0.00019852499985972827, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006157055999892691, "outcome": "passed"}, "call": {"duration": 2.651920005999955, "outcome": "passed"}, "teardown": {"duration": 0.00012935300014760287, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "add_product_tool"}, "setup": {"duration": 0.006143595000139612, "outcome": "passed"}, "call": {"duration": 4.210402712999894, "outcome": "passed"}, "teardown": {"duration": 0.00012955300007888582, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006098983999891061, "outcome": "passed"}, "call": {"duration": 7.069841765000092, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 445, "message": "AssertionError: Expected arguments '{'name': 'Team Building', 'date': '2025-03-03', 'time': '10:00', 'location': 'Main Conference Room', 'participants': ['Alice', 'Bob', 'Charlie']}', got '{'date': '2025-03-03', 'location': 'Main Conference Room', 'name': 'Team Building', 'participants': '[\"Alice\", \"Bob\", \"Charlie\"]', 'time': '10:00'}'\nassert {'date': '202...arlie\"]', ...} == {'date': '202...harlie'], ...}\n  \n  Omitting 4 identical items, use -vv to show\n  Differing items:\n  {'participants': '[\"Alice\", \"Bob\", \"Charlie\"]'} != {'participants': ['Alice', 'Bob', 'Charlie']}\n  \n  Full diff:\n    {\n        'date': '2025-03-03',\n        'location': 'Main Conference Room',\n        'name': 'Team Building',\n  +     'participants': '[\"Alice\", \"Bob\", \"Charlie\"]',\n  -     'participants': [\n  -         'Alice',\n  -         'Bob',\n  -         'Charlie',\n  -     ],\n        'time': '10:00',\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 445, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fcc028cfb90>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'name': 'Team Building', 'date': '2025-03-03', 'time': '10:00', 'location': 'Main Conference Room', 'participants': ['Alice', 'Bob', 'Charlie']}', got '{'date': '2025-03-03', 'location': 'Main Conference Room', 'name': 'Team Building', 'participants': '[\"Alice\", \"Bob\", \"Charlie\"]', 'time': '10:00'}'\nE               assert {'date': '202...arlie\"]', ...} == {'date': '202...harlie'], ...}\nE                 \nE                 Omitting 4 identical items, use -vv to show\nE                 Differing items:\nE                 {'participants': '[\"Alice\", \"Bob\", \"Charlie\"]'} != {'participants': ['Alice', 'Bob', 'Charlie']}\nE                 \nE                 Full diff:\nE                   {\nE                       'date': '2025-03-03',\nE                       'location': 'Main Conference Room',\nE                       'name': 'Team Building',\nE                 +     'participants': '[\"Alice\", \"Bob\", \"Charlie\"]',\nE                 -     'participants': [\nE                 -         'Alice',\nE                 -         'Bob',\nE                 -         'Charlie',\nE                 -     ],\nE                       'time': '10:00',\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:445: AssertionError"}, "teardown": {"duration": 0.0001443339999696036, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006400691000180814, "outcome": "passed"}, "call": {"duration": 4.923393104999832, "outcome": "passed"}, "teardown": {"duration": 0.00013374399986787466, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.0062054669999724865, "outcome": "passed"}, "call": {"duration": 1.385857101000056, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 493, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fcc023d5790>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n>           stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:493: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fcc023d5790>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\n\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00012263300004633493, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.005954791000021942, "outcome": "passed"}, "call": {"duration": 1.237157070999956, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 493, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]>>\nopenai_client = <openai.OpenAI object at 0x7fcc02995010>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'weather_tool_then_text', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'location': 'San Francisco...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n>           stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:493: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fcc02995010>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\n\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00014012299993737543, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "add_product_tool"}, "setup": {"duration": 0.005906189999905109, "outcome": "passed"}, "call": {"duration": 1.4486915070001487, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 493, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fcc0243bd90>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'add_product_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'inStock': True, 'name': 'Widget...}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': 'Successfully added product with id: 123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n>           stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:493: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fcc0243bd90>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\n\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00014852300000711693, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.0062220530001013685, "outcome": "passed"}, "call": {"duration": 1.3325147729999571, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 493, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fcc028aaf50>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n>           stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:493: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fcc028aaf50>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\n\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00012018300003546756, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006026367999993454, "outcome": "passed"}, "call": {"duration": 1.3106136710000555, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 493, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1008, "message": "in _request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1057, "message": "in _retry_request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "InternalServerError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fcc029524d0>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ramalama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n>           stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:493: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1008: in _request\n    return self._retry_request(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1057: in _retry_request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fcc029524d0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.InternalServerError: Error code: 500 - {'error': {'code': 500, 'message': 'Cannot use tools with stream', 'type': 'server_error'}}\n\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1023: InternalServerError"}, "teardown": {"duration": 0.00012286299988772953, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "stream=False"}, "setup": {"duration": 0.006544350000012855, "outcome": "passed"}, "call": {"duration": 0.00010681200001272373, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama3.3:70b-instruct-q4_K_M on provider ramalama based on config.')"}, "teardown": {"duration": 0.00010450199988554232, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "stream=True"}, "setup": {"duration": 0.006898808999949324, "outcome": "passed"}, "call": {"duration": 9.458199997425254e-05, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama3.3:70b-instruct-q4_K_M on provider ramalama based on config.')"}, "teardown": {"duration": 0.0005766129997937242, "outcome": "passed"}}]}