{"created": 1747448489.3739197, "duration": 169.13670301437378, "exitcode": 1, "root": "/actions-runner/_work/lls-openai-client/lls-openai-client", "environment": {}, "summary": {"passed": 31, "failed": 7, "total": 38, "collected": 38}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "earth"}, "setup": {"duration": 0.03245275199992648, "outcome": "passed"}, "call": {"duration": 1.6485763160000033, "outcome": "passed"}, "teardown": {"duration": 0.00017372700006035302, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "saturn"}, "setup": {"duration": 0.007006162000038785, "outcome": "passed"}, "call": {"duration": 1.0222028299999693, "outcome": "passed"}, "teardown": {"duration": 0.00015893700003744016, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "earth"}, "setup": {"duration": 0.009433008999963022, "outcome": "passed"}, "call": {"duration": 0.5200207419999288, "outcome": "passed"}, "teardown": {"duration": 0.0001591669999925216, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "saturn"}, "setup": {"duration": 0.006386674000054882, "outcome": "passed"}, "call": {"duration": 1.585511385000018, "outcome": "passed"}, "teardown": {"duration": 0.0001513669999440026, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "messages_missing"}, "setup": {"duration": 0.00640618500005985, "outcome": "passed"}, "call": {"duration": 0.0034177510000290567, "outcome": "passed"}, "teardown": {"duration": 0.00012088500000118074, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006690138000067236, "outcome": "passed"}, "call": {"duration": 0.0036238310000271667, "outcome": "passed"}, "teardown": {"duration": 0.00011752499995054677, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006667106000008971, "outcome": "passed"}, "call": {"duration": 0.003489835000095809, "outcome": "passed"}, "teardown": {"duration": 0.00011552500006928312, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.00662364400000115, "outcome": "passed"}, "call": {"duration": 0.0034720639999932246, "outcome": "passed"}, "teardown": {"duration": 0.00011745500000870379, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.006602443000019775, "outcome": "passed"}, "call": {"duration": 1.3606633100000636, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "AssertionError: assert 400 == 500\n +  where 500 = InternalServerError('Error code: 500').status_code\n +    where InternalServerError('Error code: 500') = <ExceptionInfo InternalServerError('Error code: 500') tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fbdbe5b6910>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=False,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>       assert case[\"output\"][\"error\"][\"status_code\"] == e.value.status_code\nE       AssertionError: assert 400 == 500\nE        +  where 500 = InternalServerError('Error code: 500').status_code\nE        +    where InternalServerError('Error code: 500') = <ExceptionInfo InternalServerError('Error code: 500') tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:108: AssertionError"}, "teardown": {"duration": 0.00012605599999915285, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "messages_missing"}, "setup": {"duration": 0.006118221999940943, "outcome": "passed"}, "call": {"duration": 0.0028499969999984387, "outcome": "passed"}, "teardown": {"duration": 0.00010914400002093316, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006694306999975197, "outcome": "passed"}, "call": {"duration": 0.003249384000014288, "outcome": "passed"}, "teardown": {"duration": 0.00011012499999196734, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006047499000032985, "outcome": "passed"}, "call": {"duration": 0.003271356000027481, "outcome": "passed"}, "teardown": {"duration": 0.00010629400003381306, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.00594913399993402, "outcome": "passed"}, "call": {"duration": 0.0036074499998903775, "outcome": "passed"}, "teardown": {"duration": 0.00010930500002359622, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.0058683309999878475, "outcome": "passed"}, "call": {"duration": 1.393607352999993, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "AssertionError: assert '400' in 'Error code: 500'\n +  where '400' = str(400)\n +  and   'Error code: 500' = InternalServerError('Error code: 500').message\n +    where InternalServerError('Error code: 500') = <ExceptionInfo InternalServerError('Error code: 500') tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fbdbe669450>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n            for _chunk in response:\n                pass\n>       assert str(case[\"output\"][\"error\"][\"status_code\"]) in e.value.message\nE       AssertionError: assert '400' in 'Error code: 500'\nE        +  where '400' = str(400)\nE        +  and   'Error code: 500' = InternalServerError('Error code: 500').message\nE        +    where InternalServerError('Error code: 500') = <ExceptionInfo InternalServerError('Error code: 500') tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:131: AssertionError"}, "teardown": {"duration": 0.0001281060000337675, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.006128431999968598, "outcome": "passed"}, "call": {"duration": 8.612592006, "outcome": "passed"}, "teardown": {"duration": 0.00016477700000905315, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 152, "outcome": "passed", "keywords": ["test_chat_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.00683503400000518, "outcome": "passed"}, "call": {"duration": 7.9823389389999875, "outcome": "passed"}, "teardown": {"duration": 0.000146586999903775, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "calendar"}, "setup": {"duration": 0.006380824000075336, "outcome": "passed"}, "call": {"duration": 4.023437750000085, "outcome": "passed"}, "teardown": {"duration": 0.00014989700002843165, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "math"}, "setup": {"duration": 0.0064207449999003074, "outcome": "passed"}, "call": {"duration": 17.66022856500001, "outcome": "passed"}, "teardown": {"duration": 0.00014588600004117325, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "calendar"}, "setup": {"duration": 0.006444705999911093, "outcome": "passed"}, "call": {"duration": 2.287150483000005, "outcome": "passed"}, "teardown": {"duration": 0.0001468259999910515, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "math"}, "setup": {"duration": 0.00671022799997445, "outcome": "passed"}, "call": {"duration": 15.9226067190001, "outcome": "passed"}, "teardown": {"duration": 0.00014504700004636106, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.006649234999940745, "outcome": "passed"}, "call": {"duration": 0.6945943280001075, "outcome": "passed"}, "teardown": {"duration": 0.0001531870000235358, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.006453375999967648, "outcome": "passed"}, "call": {"duration": 0.631821930000001, "outcome": "passed"}, "teardown": {"duration": 0.0001448570000093241, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.006433615999981157, "outcome": "passed"}, "call": {"duration": 2.036684052999931, "outcome": "passed"}, "teardown": {"duration": 0.00015404700002363825, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.006993840000063756, "outcome": "passed"}, "call": {"duration": 1.9313917240000364, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 318, "message": "AssertionError: Expected tool call when tool_choice='required'\nassert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 318, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fbdbe74a690>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n        _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n    \n>       assert len(tool_calls_buffer) > 0, \"Expected tool call when tool_choice='required'\"\nE       AssertionError: Expected tool call when tool_choice='required'\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/verifications/openai_api/test_chat_completion.py:318: AssertionError"}, "teardown": {"duration": 0.00014500700001462974, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 324, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.0064037850000886465, "outcome": "passed"}, "call": {"duration": 26.941408145999958, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [] is None\n +  where [] = ChatCompletionMessage(content=\"San Francisco! Known for its iconic Golden Gate Bridge, steep hills, and... foggy weather!\\n\\nSan Francisco's climate is mild and Mediterranean, with cool, wet winters and cool, dry summers. Here's what you can expect:\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect some rain, with an average of 6-7 inches (15-18 cm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and dry, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some fog, especially in the mornings and evenings.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Fog and Wind:**\\n\\n* San Francisco is known for its fog, which can roll in at any time of the year. The fog is usually thickest in the summer months, but it can also occur during other seasons.\\n* Wind is also a factor in San Francisco, with an average wind speed of 7-8 mph (11-13 km/h). Expect stronger winds during the winter months.\\n\\n**Microclimates:**\\n\\n* San Francisco has a unique geography, with hills and valleys that create microclimates. Some neighborhoods, like the Mission District and the Marina, tend to be sunnier and warmer than others, like the Haight-Ashbury and the Outer Richmond, which are often cooler and foggier.\\n\\n**Best Time to Visit:**\\n\\n* The best time to visit San Francisco is from September to November or from March to May, when the weather is mild and sunny.\\n* If you're looking for warmer weather, June to August might be a good time, but be prepared for fog and cooler temperatures.\\n\\nOverall, San Francisco's weather is known for being unpredictable and changeable, so it's always a good idea to dress in layers and be prepared for anything!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\n +    where ChatCompletionMessage(content=\"San Francisco! Known for its iconic Golden Gate Bridge, steep hills, and... foggy weather!\\n\\nSan Francisco's climate is mild and Mediterranean, with cool, wet winters and cool, dry summers. Here's what you can expect:\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect some rain, with an average of 6-7 inches (15-18 cm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and dry, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some fog, especially in the mornings and evenings.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Fog and Wind:**\\n\\n* San Francisco is known for its fog, which can roll in at any time of the year. The fog is usually thickest in the summer months, but it can also occur during other seasons.\\n* Wind is also a factor in San Francisco, with an average wind speed of 7-8 mph (11-13 km/h). Expect stronger winds during the winter months.\\n\\n**Microclimates:**\\n\\n* San Francisco has a unique geography, with hills and valleys that create microclimates. Some neighborhoods, like the Mission District and the Marina, tend to be sunnier and warmer than others, like the Haight-Ashbury and the Outer Richmond, which are often cooler and foggier.\\n\\n**Best Time to Visit:**\\n\\n* The best time to visit San Francisco is from September to November or from March to May, when the weather is mild and sunny.\\n* If you're looking for warmer weather, June to August might be a good time, but be prepared for fog and cooler temperatures.\\n\\nOverall, San Francisco's weather is known for being unpredictable and changeable, so it's always a good idea to dress in layers and be prepared for anything!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"San Francisco! Known for its iconic Golden Gate Bridge, steep hills, and... foggy weather!\\n\\nSan Francisco's climate is mild and Mediterranean, with cool, wet winters and cool, dry summers. Here's what you can expect:\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect some rain, with an average of 6-7 inches (15-18 cm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and dry, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some fog, especially in the mornings and evenings.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Fog and Wind:**\\n\\n* San Francisco is known for its fog, which can roll in at any time of the year. The fog is usually thickest in the summer months, but it can also occur during other seasons.\\n* Wind is also a factor in San Francisco, with an average wind speed of 7-8 mph (11-13 km/h). Expect stronger winds during the winter months.\\n\\n**Microclimates:**\\n\\n* San Francisco has a unique geography, with hills and valleys that create microclimates. Some neighborhoods, like the Mission District and the Marina, tend to be sunnier and warmer than others, like the Haight-Ashbury and the Outer Richmond, which are often cooler and foggier.\\n\\n**Best Time to Visit:**\\n\\n* The best time to visit San Francisco is from September to November or from March to May, when the weather is mild and sunny.\\n* If you're looking for warmer weather, June to August might be a good time, but be prepared for fog and cooler temperatures.\\n\\nOverall, San Francisco's weather is known for being unpredictable and changeable, so it's always a good idea to dress in layers and be prepared for anything!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fbdbe804290>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [] is None\nE        +  where [] = ChatCompletionMessage(content=\"San Francisco! Known for its iconic Golden Gate Bridge, steep hills, and... foggy weather!\\n\\nSan Francisco's climate is mild and Mediterranean, with cool, wet winters and cool, dry summers. Here's what you can expect:\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect some rain, with an average of 6-7 inches (15-18 cm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and dry, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some fog, especially in the mornings and evenings.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Fog and Wind:**\\n\\n* San Francisco is known for its fog, which can roll in at any time of the year. The fog is usually thickest in the summer months, but it can also occur during other seasons.\\n* Wind is also a factor in San Francisco, with an average wind speed of 7-8 mph (11-13 km/h). Expect stronger winds during the winter months.\\n\\n**Microclimates:**\\n\\n* San Francisco has a unique geography, with hills and valleys that create microclimates. Some neighborhoods, like the Mission District and the Marina, tend to be sunnier and warmer than others, like the Haight-Ashbury and the Outer Richmond, which are often cooler and foggier.\\n\\n**Best Time to Visit:**\\n\\n* The best time to visit San Francisco is from September to November or from March to May, when the weather is mild and sunny.\\n* If you're looking for warmer weather, June to August might be a good time, but be prepared for fog and cooler temperatures.\\n\\nOverall, San Francisco's weather is known for being unpredictable and changeable, so it's always a good idea to dress in layers and be prepared for anything!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\nE        +    where ChatCompletionMessage(content=\"San Francisco! Known for its iconic Golden Gate Bridge, steep hills, and... foggy weather!\\n\\nSan Francisco's climate is mild and Mediterranean, with cool, wet winters and cool, dry summers. Here's what you can expect:\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect some rain, with an average of 6-7 inches (15-18 cm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and dry, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some fog, especially in the mornings and evenings.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Fog and Wind:**\\n\\n* San Francisco is known for its fog, which can roll in at any time of the year. The fog is usually thickest in the summer months, but it can also occur during other seasons.\\n* Wind is also a factor in San Francisco, with an average wind speed of 7-8 mph (11-13 km/h). Expect stronger winds during the winter months.\\n\\n**Microclimates:**\\n\\n* San Francisco has a unique geography, with hills and valleys that create microclimates. Some neighborhoods, like the Mission District and the Marina, tend to be sunnier and warmer than others, like the Haight-Ashbury and the Outer Richmond, which are often cooler and foggier.\\n\\n**Best Time to Visit:**\\n\\n* The best time to visit San Francisco is from September to November or from March to May, when the weather is mild and sunny.\\n* If you're looking for warmer weather, June to August might be a good time, but be prepared for fog and cooler temperatures.\\n\\nOverall, San Francisco's weather is known for being unpredictable and changeable, so it's always a good idea to dress in layers and be prepared for anything!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"San Francisco! Known for its iconic Golden Gate Bridge, steep hills, and... foggy weather!\\n\\nSan Francisco's climate is mild and Mediterranean, with cool, wet winters and cool, dry summers. Here's what you can expect:\\n\\n**Seasonal Weather Patterns:**\\n\\n* **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect some rain, with an average of 6-7 inches (15-18 cm) per month.\\n* **Spring (March to May):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n* **Summer (June to August):** Cool and dry, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). Expect some fog, especially in the mornings and evenings.\\n* **Fall (September to November):** Mild and sunny, with average highs around 64\u00b0F (18\u00b0C) and lows around 52\u00b0F (11\u00b0C). Expect some fog, especially in the mornings.\\n\\n**Fog and Wind:**\\n\\n* San Francisco is known for its fog, which can roll in at any time of the year. The fog is usually thickest in the summer months, but it can also occur during other seasons.\\n* Wind is also a factor in San Francisco, with an average wind speed of 7-8 mph (11-13 km/h). Expect stronger winds during the winter months.\\n\\n**Microclimates:**\\n\\n* San Francisco has a unique geography, with hills and valleys that create microclimates. Some neighborhoods, like the Mission District and the Marina, tend to be sunnier and warmer than others, like the Haight-Ashbury and the Outer Richmond, which are often cooler and foggier.\\n\\n**Best Time to Visit:**\\n\\n* The best time to visit San Francisco is from September to November or from March to May, when the weather is mild and sunny.\\n* If you're looking for warmer weather, June to August might be a good time, but be prepared for fog and cooler temperatures.\\n\\nOverall, San Francisco's weather is known for being unpredictable and changeable, so it's always a good idea to dress in layers and be prepared for anything!\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None).message\n\ntests/verifications/openai_api/test_chat_completion.py:344: AssertionError"}, "teardown": {"duration": 0.00015630600000804407, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.006426855999961845, "outcome": "passed"}, "call": {"duration": 30.199829622999914, "outcome": "passed"}, "teardown": {"duration": 0.00015222700017147872, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006629944999986037, "outcome": "passed"}, "call": {"duration": 3.037903310000047, "outcome": "passed"}, "teardown": {"duration": 0.00014479600008598936, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.0066581260000475595, "outcome": "passed"}, "call": {"duration": 1.4829038839998248, "outcome": "passed"}, "teardown": {"duration": 0.00013711600013266434, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "add_product_tool"}, "setup": {"duration": 0.00636878299997079, "outcome": "passed"}, "call": {"duration": 2.4517023390001214, "outcome": "passed"}, "teardown": {"duration": 0.00014310599999589613, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006570322000015949, "outcome": "passed"}, "call": {"duration": 6.343721211000002, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 462, "message": "AssertionError: Expected one of ['e_123', 'event id: e_123'] in content, but got: 'create_event(name=\"Team Building\", date=\"2025-03-03\", time=\"10:00\", location=\"Main Conference Room\", participants=[\"Alice\", \"Bob\", \"Charlie\"])'\nassert False\n +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fbdbe5bc2e0>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 462, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fbdbe75c410>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert assistant_message.content is not None, \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]  # This is now a list\n                content_lower = assistant_message.content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{assistant_message.content}'\"\n                )\nE               AssertionError: Expected one of ['e_123', 'event id: e_123'] in content, but got: 'create_event(name=\"Team Building\", date=\"2025-03-03\", time=\"10:00\", location=\"Main Conference Room\", participants=[\"Alice\", \"Bob\", \"Charlie\"])'\nE               assert False\nE                +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fbdbe5bc2e0>)\n\ntests/verifications/openai_api/test_chat_completion.py:462: AssertionError"}, "teardown": {"duration": 0.00014733700004398997, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0063778930000353284, "outcome": "passed"}, "call": {"duration": 2.66943964699999, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 1 tool calls, but got 0\nassert 0 == 1\n +  where 0 = len(([] or []))\n +    where [] = ChatCompletionMessage(content='getMonthlyExpenseSummary(month=2, year=2024)', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fbdbeb6b290>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 0\nE           assert 0 == 1\nE            +  where 0 = len(([] or []))\nE            +    where [] = ChatCompletionMessage(content='getMonthlyExpenseSummary(month=2, year=2024)', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00015130699989640561, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.0063800740001624945, "outcome": "passed"}, "call": {"duration": 2.139222208000092, "outcome": "passed"}, "teardown": {"duration": 0.00014509599986922694, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006401723999942988, "outcome": "passed"}, "call": {"duration": 1.4509011279999413, "outcome": "passed"}, "teardown": {"duration": 0.00014685699989058776, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "add_product_tool"}, "setup": {"duration": 0.006275829000060185, "outcome": "passed"}, "call": {"duration": 2.3007640529999662, "outcome": "passed"}, "teardown": {"duration": 0.00016205700012505986, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006317711000065174, "outcome": "passed"}, "call": {"duration": 4.2614807400000245, "outcome": "passed"}, "teardown": {"duration": 0.00014839699997537537, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.0063363619999563525, "outcome": "passed"}, "call": {"duration": 2.5852615779999724, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 1 tool calls, but got 0\nassert 0 == 1\n +  where 0 = len(([] or []))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fbdbe5cfe10>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 1 tool calls, but got 0\nE           assert 0 == 1\nE            +  where 0 = len(([] or []))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.00014755699999113858, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=False]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "stream=False"}, "setup": {"duration": 0.0070515540000997134, "outcome": "passed"}, "call": {"duration": 5.456049611000026, "outcome": "passed"}, "teardown": {"duration": 0.00015408700005536957, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=True]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=True]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "stream=True"}, "setup": {"duration": 0.0070693939999273425, "outcome": "passed"}, "call": {"duration": 7.779665368999986, "outcome": "passed"}, "teardown": {"duration": 0.0007069910000154778, "outcome": "passed"}}]}