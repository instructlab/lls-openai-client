{"created": 1748139720.2810173, "duration": 165.10866713523865, "exitcode": 1, "root": "/actions-runner/_work/lls-openai-client/lls-openai-client", "environment": {}, "summary": {"passed": 31, "failed": 7, "total": 38, "collected": 38}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "earth"}, "setup": {"duration": 0.08728274399993552, "outcome": "passed"}, "call": {"duration": 1.6623026720000098, "outcome": "passed"}, "teardown": {"duration": 0.00013566199993420014, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "saturn"}, "setup": {"duration": 0.00023429400005170464, "outcome": "passed"}, "call": {"duration": 1.0292318140000134, "outcome": "passed"}, "teardown": {"duration": 0.00012901299999157345, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "earth"}, "setup": {"duration": 0.00020642399999815098, "outcome": "passed"}, "call": {"duration": 3.1838172620000478, "outcome": "passed"}, "teardown": {"duration": 0.00012242200000400771, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "saturn"}, "setup": {"duration": 0.00022395400003460963, "outcome": "passed"}, "call": {"duration": 1.0282358569999133, "outcome": "passed"}, "teardown": {"duration": 0.00011174200005825696, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "messages_missing"}, "setup": {"duration": 0.00020845400001689995, "outcome": "passed"}, "call": {"duration": 0.003295981000064785, "outcome": "passed"}, "teardown": {"duration": 0.0001066920000312166, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.00017445299999963026, "outcome": "passed"}, "call": {"duration": 0.0032335999999304477, "outcome": "passed"}, "teardown": {"duration": 8.219199992254289e-05, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.00016941399996994733, "outcome": "passed"}, "call": {"duration": 0.003233779999959552, "outcome": "passed"}, "teardown": {"duration": 8.007199994608527e-05, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.00019239300002027448, "outcome": "passed"}, "call": {"duration": 0.0032006290000481386, "outcome": "passed"}, "teardown": {"duration": 8.244100001775223e-05, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.00018443399994794163, "outcome": "passed"}, "call": {"duration": 1.2865404330000274, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "AssertionError: assert 400 == 500\n +  where 500 = InternalServerError('Error code: 500').status_code\n +    where InternalServerError('Error code: 500') = <ExceptionInfo InternalServerError('Error code: 500') tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 108, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fa1148e2450>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=False,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>       assert case[\"output\"][\"error\"][\"status_code\"] == e.value.status_code\nE       AssertionError: assert 400 == 500\nE        +  where 500 = InternalServerError('Error code: 500').status_code\nE        +    where InternalServerError('Error code: 500') = <ExceptionInfo InternalServerError('Error code: 500') tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:108: AssertionError"}, "teardown": {"duration": 0.00010222200000953308, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "messages_missing"}, "setup": {"duration": 0.0001924330000520058, "outcome": "passed"}, "call": {"duration": 0.002592548000052375, "outcome": "passed"}, "teardown": {"duration": 9.634200000618875e-05, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.00018865399999867805, "outcome": "passed"}, "call": {"duration": 0.0031273270000156117, "outcome": "passed"}, "teardown": {"duration": 9.56220000034591e-05, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.00020472400001381175, "outcome": "passed"}, "call": {"duration": 0.003335972000058973, "outcome": "passed"}, "teardown": {"duration": 9.300200008510728e-05, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.0001987240000289603, "outcome": "passed"}, "call": {"duration": 0.0033834719999958907, "outcome": "passed"}, "teardown": {"duration": 9.453200004827522e-05, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.00017908300003455224, "outcome": "passed"}, "call": {"duration": 1.4158095140001024, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "AssertionError: assert '400' in 'Error code: 500'\n +  where '400' = str(400)\n +  and   'Error code: 500' = InternalServerError('Error code: 500').message\n +    where InternalServerError('Error code: 500') = <ExceptionInfo InternalServerError('Error code: 500') tblen=10>.value"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 131, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fa1148e2450>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n            for _chunk in response:\n                pass\n>       assert str(case[\"output\"][\"error\"][\"status_code\"]) in e.value.message\nE       AssertionError: assert '400' in 'Error code: 500'\nE        +  where '400' = str(400)\nE        +  and   'Error code: 500' = InternalServerError('Error code: 500').message\nE        +    where InternalServerError('Error code: 500') = <ExceptionInfo InternalServerError('Error code: 500') tblen=10>.value\n\ntests/verifications/openai_api/test_chat_completion.py:131: AssertionError"}, "teardown": {"duration": 9.793199990326684e-05, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 133, "outcome": "passed", "keywords": ["test_chat_non_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.00021395399994617037, "outcome": "passed"}, "call": {"duration": 10.467427333000046, "outcome": "passed"}, "teardown": {"duration": 0.000114852000024257, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 152, "outcome": "passed", "keywords": ["test_chat_streaming_image[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.0002092839999932039, "outcome": "passed"}, "call": {"duration": 7.665599602000043, "outcome": "passed"}, "teardown": {"duration": 0.0001111920000766986, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "calendar"}, "setup": {"duration": 0.00021002500000122382, "outcome": "passed"}, "call": {"duration": 3.7692054180000696, "outcome": "passed"}, "teardown": {"duration": 0.00012076300004082441, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "math"}, "setup": {"duration": 0.00020968500007256807, "outcome": "passed"}, "call": {"duration": 15.574298290999991, "outcome": "passed"}, "teardown": {"duration": 0.0001481850000573104, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "calendar"}, "setup": {"duration": 0.00030458900005214673, "outcome": "passed"}, "call": {"duration": 2.1252492910000456, "outcome": "passed"}, "teardown": {"duration": 0.00012743299998874136, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "math"}, "setup": {"duration": 0.00021436499991978053, "outcome": "passed"}, "call": {"duration": 17.619183804000045, "outcome": "passed"}, "teardown": {"duration": 0.00011399300001357915, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.00021084399998017034, "outcome": "passed"}, "call": {"duration": 0.6771007340000779, "outcome": "passed"}, "teardown": {"duration": 0.00011723299996901915, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.00019931399992856313, "outcome": "passed"}, "call": {"duration": 0.5821646769999234, "outcome": "passed"}, "teardown": {"duration": 9.820200000376644e-05, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 273, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.00019931400004224997, "outcome": "passed"}, "call": {"duration": 0.2904272349999246, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 293, "message": "AssertionError: Expected tool call when tool_choice='required'\nassert 0 > 0\n +  where 0 = len([])\n +    where [] = ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\n +      where ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 293, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fa1148e2450>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert len(response.choices[0].message.tool_calls) > 0, \"Expected tool call when tool_choice='required'\"\nE       AssertionError: Expected tool call when tool_choice='required'\nE       assert 0 > 0\nE        +  where 0 = len([])\nE        +    where [] = ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\nE        +      where ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None).message\n\ntests/verifications/openai_api/test_chat_completion.py:293: AssertionError"}, "teardown": {"duration": 0.00013767199993708346, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 297, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.00020627399999284535, "outcome": "passed"}, "call": {"duration": 0.1848324729999149, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 318, "message": "AssertionError: Expected tool call when tool_choice='required'\nassert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 318, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fa1148e2450>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n        _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n    \n>       assert len(tool_calls_buffer) > 0, \"Expected tool call when tool_choice='required'\"\nE       AssertionError: Expected tool call when tool_choice='required'\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/verifications/openai_api/test_chat_completion.py:318: AssertionError"}, "teardown": {"duration": 9.910200003560021e-05, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 324, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.00018929400005163188, "outcome": "passed"}, "call": {"duration": 28.710230681999974, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [] is None\n +  where [] = ChatCompletionMessage(content=\"San Francisco, California, is known for its mild and pleasant weather year-round, often referred to as a Mediterranean climate. Here's a breakdown of the typical weather conditions:\\n\\n**Seasonal Weather Patterns:**\\n\\n1. **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect occasional rain showers, but not heavy or prolonged.\\n2. **Spring (March to May):** Mild and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 50\u00b0F (10\u00b0C). This is a great time to visit, with fewer crowds and pleasant weather.\\n3. **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). While it's not hot, the fog can roll in, especially in the mornings and evenings.\\n4. **Fall (September to November):** Warm and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 55\u00b0F (13\u00b0C). This is another great time to visit, with comfortable temperatures and fewer tourists.\\n\\n**Microclimates:**\\n\\nSan Francisco's unique geography creates microclimates, which can result in varying weather conditions across different neighborhoods:\\n\\n* **The Haight-Ashbury and the Mission District:** Tend to be sunnier and warmer than other areas.\\n* **Fisherman's Wharf and the Embarcadero:** Can be cooler and foggier, especially in the mornings.\\n* **The Hills:** Areas like Twin Peaks, Nob Hill, and Pacific Heights can be cooler and windier than lower-lying neighborhoods.\\n\\n**Other Weather Notes:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. It's not uncommon for the fog to clear by midday, only to return in the evenings.\\n* **Wind:** Expect gentle breezes, especially in the afternoons and evenings.\\n* **Rain:** San Francisco receives most of its rainfall between November and March, with an average annual rainfall of around 20 inches (508 mm).\\n\\nOverall, San Francisco's weather is generally mild and pleasant, making it a great destination to visit year-round. Just be prepared for some fog and cooler temperatures, especially in the mornings and evenings.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\n +    where ChatCompletionMessage(content=\"San Francisco, California, is known for its mild and pleasant weather year-round, often referred to as a Mediterranean climate. Here's a breakdown of the typical weather conditions:\\n\\n**Seasonal Weather Patterns:**\\n\\n1. **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect occasional rain showers, but not heavy or prolonged.\\n2. **Spring (March to May):** Mild and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 50\u00b0F (10\u00b0C). This is a great time to visit, with fewer crowds and pleasant weather.\\n3. **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). While it's not hot, the fog can roll in, especially in the mornings and evenings.\\n4. **Fall (September to November):** Warm and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 55\u00b0F (13\u00b0C). This is another great time to visit, with comfortable temperatures and fewer tourists.\\n\\n**Microclimates:**\\n\\nSan Francisco's unique geography creates microclimates, which can result in varying weather conditions across different neighborhoods:\\n\\n* **The Haight-Ashbury and the Mission District:** Tend to be sunnier and warmer than other areas.\\n* **Fisherman's Wharf and the Embarcadero:** Can be cooler and foggier, especially in the mornings.\\n* **The Hills:** Areas like Twin Peaks, Nob Hill, and Pacific Heights can be cooler and windier than lower-lying neighborhoods.\\n\\n**Other Weather Notes:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. It's not uncommon for the fog to clear by midday, only to return in the evenings.\\n* **Wind:** Expect gentle breezes, especially in the afternoons and evenings.\\n* **Rain:** San Francisco receives most of its rainfall between November and March, with an average annual rainfall of around 20 inches (508 mm).\\n\\nOverall, San Francisco's weather is generally mild and pleasant, making it a great destination to visit year-round. Just be prepared for some fog and cooler temperatures, especially in the mornings and evenings.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"San Francisco, California, is known for its mild and pleasant weather year-round, often referred to as a Mediterranean climate. Here's a breakdown of the typical weather conditions:\\n\\n**Seasonal Weather Patterns:**\\n\\n1. **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect occasional rain showers, but not heavy or prolonged.\\n2. **Spring (March to May):** Mild and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 50\u00b0F (10\u00b0C). This is a great time to visit, with fewer crowds and pleasant weather.\\n3. **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). While it's not hot, the fog can roll in, especially in the mornings and evenings.\\n4. **Fall (September to November):** Warm and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 55\u00b0F (13\u00b0C). This is another great time to visit, with comfortable temperatures and fewer tourists.\\n\\n**Microclimates:**\\n\\nSan Francisco's unique geog..., which can result in varying weather conditions across different neighborhoods:\\n\\n* **The Haight-Ashbury and the Mission District:** Tend to be sunnier and warmer than other areas.\\n* **Fisherman's Wharf and the Embarcadero:** Can be cooler and foggier, especially in the mornings.\\n* **The Hills:** Areas like Twin Peaks, Nob Hill, and Pacific Heights can be cooler and windier than lower-lying neighborhoods.\\n\\n**Other Weather Notes:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. It's not uncommon for the fog to clear by midday, only to return in the evenings.\\n* **Wind:** Expect gentle breezes, especially in the afternoons and evenings.\\n* **Rain:** San Francisco receives most of its rainfall between November and March, with an average annual rainfall of around 20 inches (508 mm).\\n\\nOverall, San Francisco's weather is generally mild and pleasant, making it a great destination to visit year-round. Just be prepared for some fog and cooler temperatures, especially in the mornings and evenings.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fa1148e2450>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [] is None\nE        +  where [] = ChatCompletionMessage(content=\"San Francisco, California, is known for its mild and pleasant weather year-round, often referred to as a Mediterranean climate. Here's a breakdown of the typical weather conditions:\\n\\n**Seasonal Weather Patterns:**\\n\\n1. **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect occasional rain showers, but not heavy or prolonged.\\n2. **Spring (March to May):** Mild and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 50\u00b0F (10\u00b0C). This is a great time to visit, with fewer crowds and pleasant weather.\\n3. **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). While it's not hot, the fog can roll in, especially in the mornings and evenings.\\n4. **Fall (September to November):** Warm and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 55\u00b0F (13\u00b0C). This is another great time to visit, with comfortable temperatures and fewer tourists.\\n\\n**Microclimates:**\\n\\nSan Francisco's unique geography creates microclimates, which can result in varying weather conditions across different neighborhoods:\\n\\n* **The Haight-Ashbury and the Mission District:** Tend to be sunnier and warmer than other areas.\\n* **Fisherman's Wharf and the Embarcadero:** Can be cooler and foggier, especially in the mornings.\\n* **The Hills:** Areas like Twin Peaks, Nob Hill, and Pacific Heights can be cooler and windier than lower-lying neighborhoods.\\n\\n**Other Weather Notes:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. It's not uncommon for the fog to clear by midday, only to return in the evenings.\\n* **Wind:** Expect gentle breezes, especially in the afternoons and evenings.\\n* **Rain:** San Francisco receives most of its rainfall between November and March, with an average annual rainfall of around 20 inches (508 mm).\\n\\nOverall, San Francisco's weather is generally mild and pleasant, making it a great destination to visit year-round. Just be prepared for some fog and cooler temperatures, especially in the mornings and evenings.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\nE        +    where ChatCompletionMessage(content=\"San Francisco, California, is known for its mild and pleasant weather year-round, often referred to as a Mediterranean climate. Here's a breakdown of the typical weather conditions:\\n\\n**Seasonal Weather Patterns:**\\n\\n1. **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect occasional rain showers, but not heavy or prolonged.\\n2. **Spring (March to May):** Mild and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 50\u00b0F (10\u00b0C). This is a great time to visit, with fewer crowds and pleasant weather.\\n3. **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). While it's not hot, the fog can roll in, especially in the mornings and evenings.\\n4. **Fall (September to November):** Warm and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 55\u00b0F (13\u00b0C). This is another great time to visit, with comfortable temperatures and fewer tourists.\\n\\n**Microclimates:**\\n\\nSan Francisco's unique geography creates microclimates, which can result in varying weather conditions across different neighborhoods:\\n\\n* **The Haight-Ashbury and the Mission District:** Tend to be sunnier and warmer than other areas.\\n* **Fisherman's Wharf and the Embarcadero:** Can be cooler and foggier, especially in the mornings.\\n* **The Hills:** Areas like Twin Peaks, Nob Hill, and Pacific Heights can be cooler and windier than lower-lying neighborhoods.\\n\\n**Other Weather Notes:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. It's not uncommon for the fog to clear by midday, only to return in the evenings.\\n* **Wind:** Expect gentle breezes, especially in the afternoons and evenings.\\n* **Rain:** San Francisco receives most of its rainfall between November and March, with an average annual rainfall of around 20 inches (508 mm).\\n\\nOverall, San Francisco's weather is generally mild and pleasant, making it a great destination to visit year-round. Just be prepared for some fog and cooler temperatures, especially in the mornings and evenings.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"San Francisco, California, is known for its mild and pleasant weather year-round, often referred to as a Mediterranean climate. Here's a breakdown of the typical weather conditions:\\n\\n**Seasonal Weather Patterns:**\\n\\n1. **Winter (December to February):** Cool and wet, with average highs around 57\u00b0F (14\u00b0C) and lows around 47\u00b0F (8\u00b0C). Expect occasional rain showers, but not heavy or prolonged.\\n2. **Spring (March to May):** Mild and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 50\u00b0F (10\u00b0C). This is a great time to visit, with fewer crowds and pleasant weather.\\n3. **Summer (June to August):** Cool and foggy, with average highs around 67\u00b0F (19\u00b0C) and lows around 54\u00b0F (12\u00b0C). While it's not hot, the fog can roll in, especially in the mornings and evenings.\\n4. **Fall (September to November):** Warm and sunny, with average highs in the mid-60s to low 70s Fahrenheit (18\u00b0C to 22\u00b0C) and lows around 55\u00b0F (13\u00b0C). This is another great time to visit, with comfortable temperatures and fewer tourists.\\n\\n**Microclimates:**\\n\\nSan Francisco's unique geog..., which can result in varying weather conditions across different neighborhoods:\\n\\n* **The Haight-Ashbury and the Mission District:** Tend to be sunnier and warmer than other areas.\\n* **Fisherman's Wharf and the Embarcadero:** Can be cooler and foggier, especially in the mornings.\\n* **The Hills:** Areas like Twin Peaks, Nob Hill, and Pacific Heights can be cooler and windier than lower-lying neighborhoods.\\n\\n**Other Weather Notes:**\\n\\n* **Fog:** San Francisco is famous for its fog, which can roll in at any time of the year. It's not uncommon for the fog to clear by midday, only to return in the evenings.\\n* **Wind:** Expect gentle breezes, especially in the afternoons and evenings.\\n* **Rain:** San Francisco receives most of its rainfall between November and March, with an average annual rainfall of around 20 inches (508 mm).\\n\\nOverall, San Francisco's weather is generally mild and pleasant, making it a great destination to visit year-round. Just be prepared for some fog and cooler temperatures, especially in the mornings and evenings.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None).message\n\ntests/verifications/openai_api/test_chat_completion.py:344: AssertionError"}, "teardown": {"duration": 0.00012269200010450731, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "case0"}, "setup": {"duration": 0.00020692499992946978, "outcome": "passed"}, "call": {"duration": 28.226520490999974, "outcome": "passed"}, "teardown": {"duration": 0.00011369200001354329, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.0002280840001276374, "outcome": "passed"}, "call": {"duration": 2.0701321080000525, "outcome": "passed"}, "teardown": {"duration": 0.00010851199999706296, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.00022269399983088078, "outcome": "passed"}, "call": {"duration": 1.4128519750001942, "outcome": "passed"}, "teardown": {"duration": 0.00012184299998807546, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "add_product_tool"}, "setup": {"duration": 0.00020079399996575376, "outcome": "passed"}, "call": {"duration": 2.0904272489999585, "outcome": "passed"}, "teardown": {"duration": 0.00010998200014000759, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.00022458399985225697, "outcome": "passed"}, "call": {"duration": 4.45413785400001, "outcome": "passed"}, "teardown": {"duration": 0.00011563299995032139, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.00023539399990113452, "outcome": "passed"}, "call": {"duration": 2.0538799749999725, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 462, "message": "AssertionError: Expected one of ['1000', '$1,000', '1,000'] in content, but got: 'I am unable to execute this task as it exceeds the limitations of the functions at my disposal.'\nassert False\n +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fa1149ac9e0>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 462, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fa1148e2450>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert assistant_message.content is not None, \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]  # This is now a list\n                content_lower = assistant_message.content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{assistant_message.content}'\"\n                )\nE               AssertionError: Expected one of ['1000', '$1,000', '1,000'] in content, but got: 'I am unable to execute this task as it exceeds the limitations of the functions at my disposal.'\nE               assert False\nE                +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fa1149ac9e0>)\n\ntests/verifications/openai_api/test_chat_completion.py:462: AssertionError"}, "teardown": {"duration": 0.00013124199995218078, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.00020960300003025623, "outcome": "passed"}, "call": {"duration": 1.9789795680001134, "outcome": "passed"}, "teardown": {"duration": 0.0001074320000498119, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.00021032299991929904, "outcome": "passed"}, "call": {"duration": 1.4513278770000397, "outcome": "passed"}, "teardown": {"duration": 0.00011073199993916205, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "add_product_tool"}, "setup": {"duration": 0.0002106130000356643, "outcome": "passed"}, "call": {"duration": 2.2411197229998834, "outcome": "passed"}, "teardown": {"duration": 0.00010870100004467531, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.00020995300019421848, "outcome": "passed"}, "call": {"duration": 4.095036776999905, "outcome": "passed"}, "teardown": {"duration": 0.00013949300000604126, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.00021158399999876565, "outcome": "passed"}, "call": {"duration": 2.0170992299999853, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 545, "message": "AssertionError: Expected one of ['1000', '$1,000', '1,000'] in content, but got: 'I am unable to execute this task as it exceeds the limitations of the functions I have at hand.'\nassert False\n +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fa1149adb60>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 545, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fa1148e2450>\nmodel = 'RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', provider = 'vllm'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert accumulated_content is not None and accumulated_content != \"\", \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]\n                content_lower = accumulated_content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{accumulated_content}'\"\n                )\nE               AssertionError: Expected one of ['1000', '$1,000', '1,000'] in content, but got: 'I am unable to execute this task as it exceeds the limitations of the functions I have at hand.'\nE               assert False\nE                +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fa1149adb60>)\n\ntests/verifications/openai_api/test_chat_completion.py:545: AssertionError"}, "teardown": {"duration": 0.00012221300016790337, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=False]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=False]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "stream=False"}, "setup": {"duration": 0.0008406260001265764, "outcome": "passed"}, "call": {"duration": 5.527845558999843, "outcome": "passed"}, "teardown": {"duration": 0.00012060200015184819, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=True]", "lineno": 549, "outcome": "passed", "keywords": ["test_chat_multi_turn_multiple_images[RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=True]", "parametrize", "pytestmark", "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic", "case_id": "stream=True"}, "setup": {"duration": 0.0008658609999656619, "outcome": "passed"}, "call": {"duration": 9.71782724500008, "outcome": "passed"}, "teardown": {"duration": 0.0003497569998671679, "outcome": "passed"}}]}