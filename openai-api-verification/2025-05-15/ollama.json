{"created": 1747308951.8969765, "duration": 179.85947251319885, "exitcode": 1, "root": "/actions-runner/_work/lls-openai-client/lls-openai-client", "environment": {}, "summary": {"passed": 20, "failed": 14, "skipped": 4, "total": 38, "collected": 38}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "earth"}, "setup": {"duration": 0.03318193699999483, "outcome": "passed"}, "call": {"duration": 3.86262568799998, "outcome": "passed"}, "teardown": {"duration": 0.00017125299996223475, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "saturn"}, "setup": {"duration": 0.007023742000001221, "outcome": "passed"}, "call": {"duration": 4.294397428000025, "outcome": "passed"}, "teardown": {"duration": 0.00016660500000398315, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "earth"}, "setup": {"duration": 0.007022353999957431, "outcome": "passed"}, "call": {"duration": 4.304148462000001, "outcome": "passed"}, "teardown": {"duration": 0.00015750400001479647, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "saturn"}, "setup": {"duration": 0.006930420999992748, "outcome": "passed"}, "call": {"duration": 2.745364139000003, "outcome": "passed"}, "teardown": {"duration": 0.00016332300003796263, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_missing"}, "setup": {"duration": 0.007138725999993767, "outcome": "passed"}, "call": {"duration": 0.0037344959999927596, "outcome": "passed"}, "teardown": {"duration": 0.00014982300001520343, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.0069154820000107975, "outcome": "passed"}, "call": {"duration": 44.00703664400004, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fa037d6c110>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_role_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'fake_role'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.00015072299999019378, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.0064644860000271365, "outcome": "passed"}, "call": {"duration": 4.173928561000025, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fa037d41b50>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'invalid'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.0001627530000405386, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.0065460870000038085, "outcome": "passed"}, "call": {"duration": 4.642848587999993, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fa037ead690>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.00015415299998267074, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "lineno": 89, "outcome": "failed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.006300720000012916, "outcome": "passed"}, "call": {"duration": 1.4271546030000195, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 100, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fa038004a10>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:100: Failed"}, "teardown": {"duration": 0.00015767299998969975, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_missing"}, "setup": {"duration": 0.0067789899999866066, "outcome": "passed"}, "call": {"duration": 0.002561199000012948, "outcome": "passed"}, "teardown": {"duration": 0.00012844299999414943, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006647516999976233, "outcome": "passed"}, "call": {"duration": 11.722899729999995, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fa037d41010>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'messages_role_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'fake_role'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00014280300001701107, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006294244000002891, "outcome": "passed"}, "call": {"duration": 5.314486371999976, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fa037f888d0>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'invalid'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00014259200003152728, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.006127779999985705, "outcome": "passed"}, "call": {"duration": 4.643743850000021, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fa037cd5c90>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.00016376299998910326, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "lineno": 110, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.006124158999966767, "outcome": "passed"}, "call": {"duration": 1.56170548099999, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed: DID NOT RAISE <class 'openai.APIError'>"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 121, "message": "Failed"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fa037caf510>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n>       with pytest.raises(APIError) as e:\nE       Failed: DID NOT RAISE <class 'openai.APIError'>\n\ntests/verifications/openai_api/test_chat_completion.py:121: Failed"}, "teardown": {"duration": 0.0001678529999935563, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 133, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.00633311199999298, "outcome": "passed"}, "call": {"duration": 0.00011914200001683639, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 142, 'Skipped: Skipping test_chat_non_streaming_image for model llama3.3:70b-instruct-q4_K_M on provider ollama based on config.')"}, "teardown": {"duration": 0.00010375199997270101, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 152, "outcome": "skipped", "keywords": ["test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006582416000014746, "outcome": "passed"}, "call": {"duration": 9.728200001291043e-05, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 161, 'Skipped: Skipping test_chat_streaming_image for model llama3.3:70b-instruct-q4_K_M on provider ollama based on config.')"}, "teardown": {"duration": 0.00010503199996492185, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "calendar"}, "setup": {"duration": 0.006014965999952437, "outcome": "passed"}, "call": {"duration": 1.8842395570000008, "outcome": "passed"}, "teardown": {"duration": 0.00011580199998206808, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "math"}, "setup": {"duration": 0.00673059899997952, "outcome": "passed"}, "call": {"duration": 23.935091215, "outcome": "passed"}, "teardown": {"duration": 0.0001639829999930953, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "calendar"}, "setup": {"duration": 0.0068677179999667715, "outcome": "passed"}, "call": {"duration": 1.7361632060000147, "outcome": "passed"}, "teardown": {"duration": 0.0001440820000198073, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "math"}, "setup": {"duration": 0.0068033009999908245, "outcome": "passed"}, "call": {"duration": 9.370992677000004, "outcome": "passed"}, "teardown": {"duration": 0.00015282300000762916, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.007092075999992176, "outcome": "passed"}, "call": {"duration": 1.4756555539999567, "outcome": "passed"}, "teardown": {"duration": 0.00015013300003374752, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.007037335000006806, "outcome": "passed"}, "call": {"duration": 1.3243143490000193, "outcome": "passed"}, "teardown": {"duration": 0.00016459400001167523, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.007137577000037254, "outcome": "passed"}, "call": {"duration": 1.3341260740000394, "outcome": "passed"}, "teardown": {"duration": 0.0001368320000096901, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 297, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.013749853999968309, "outcome": "passed"}, "call": {"duration": 1.3261719430000198, "outcome": "passed"}, "teardown": {"duration": 0.0001482320000150139, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 324, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006899282000006224, "outcome": "passed"}, "call": {"duration": 1.330124555999987, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [ChatCompletionMessageToolCall(id='call_yv0pans7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] is None\n +  where [ChatCompletionMessageToolCall(id='call_yv0pans7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yv0pans7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]).tool_calls\n +    where ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yv0pans7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]) = Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yv0pans7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)])).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 344, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fa037c6d3d0>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [ChatCompletionMessageToolCall(id='call_yv0pans7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] is None\nE        +  where [ChatCompletionMessageToolCall(id='call_yv0pans7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yv0pans7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]).tool_calls\nE        +    where ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yv0pans7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]) = Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_yv0pans7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)])).message\n\ntests/verifications/openai_api/test_chat_completion.py:344: AssertionError"}, "teardown": {"duration": 0.0001298320000273634, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 347, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006215349999990849, "outcome": "passed"}, "call": {"duration": 1.2673968369999784, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 371, "message": "AssertionError: Expected no tool call chunks when tool_choice='none'\nassert not [ChoiceDeltaToolCall(index=0, id='call_tx3l45h8', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')]\n +  where [ChoiceDeltaToolCall(index=0, id='call_tx3l45h8', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')] = ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_tx3l45h8', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 371, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fa037c88710>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n    \n        content = \"\"\n        for chunk in stream:\n            delta = chunk.choices[0].delta\n            if delta.content:\n                content += delta.content\n>           assert not delta.tool_calls, \"Expected no tool call chunks when tool_choice='none'\"\nE           AssertionError: Expected no tool call chunks when tool_choice='none'\nE           assert not [ChoiceDeltaToolCall(index=0, id='call_tx3l45h8', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')]\nE            +  where [ChoiceDeltaToolCall(index=0, id='call_tx3l45h8', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')] = ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_tx3l45h8', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:371: AssertionError"}, "teardown": {"duration": 0.00012711199997283984, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006182878000004166, "outcome": "passed"}, "call": {"duration": 1.5108346399999846, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_d3hdxh6p', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)]))\n +    where [ChatCompletionMessageToolCall(id='call_d3hdxh6p', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_d3hdxh6p', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fa037c8fc90>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_d3hdxh6p', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_d3hdxh6p', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_d3hdxh6p', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00016762199993536342, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006358690999945793, "outcome": "passed"}, "call": {"duration": 2.377867128999924, "outcome": "passed"}, "teardown": {"duration": 0.0001970829999891066, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "add_product_tool"}, "setup": {"duration": 0.006874938999999358, "outcome": "passed"}, "call": {"duration": 3.938429215000042, "outcome": "passed"}, "teardown": {"duration": 0.00016679299994848407, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006371224000076836, "outcome": "passed"}, "call": {"duration": 7.681825403999937, "outcome": "passed"}, "teardown": {"duration": 0.00015774300004522956, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006810156999904393, "outcome": "passed"}, "call": {"duration": 4.640277630000014, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 445, "message": "AssertionError: Expected arguments '{'month': 2, 'year': 2024}', got '{'month': 1, 'year': 2025}'\nassert {'month': 1, 'year': 2025} == {'month': 2, 'year': 2024}\n  \n  Differing items:\n  {'year': 2025} != {'year': 2024}\n  {'month': 1} != {'month': 2}\n  \n  Full diff:\n    {\n  -     'month': 2,\n  ?              ^\n  +     'month': 1,\n  ?              ^\n  -     'year': 2024,\n  ?                ^\n  +     'year': 2025,\n  ?                ^\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 445, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fa037c9ff50>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'month': 2, 'year': 2024}', got '{'month': 1, 'year': 2025}'\nE               assert {'month': 1, 'year': 2025} == {'month': 2, 'year': 2024}\nE                 \nE                 Differing items:\nE                 {'year': 2025} != {'year': 2024}\nE                 {'month': 1} != {'month': 2}\nE                 \nE                 Full diff:\nE                   {\nE                 -     'month': 2,\nE                 ?              ^\nE                 +     'month': 1,\nE                 ?              ^\nE                 -     'year': 2024,\nE                 ?                ^\nE                 +     'year': 2025,\nE                 ?                ^\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:445: AssertionError"}, "teardown": {"duration": 0.00015175300006831094, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006358640000030391, "outcome": "passed"}, "call": {"duration": 1.4625899220000065, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"location\":\"Rome, Italy\"}', 'name': 'get_weather'}, 'id': 'call_pbn037uk', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 516, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fa037cd0150>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"location\":\"Rome, Italy\"}', 'name': 'get_weather'}, 'id': 'call_pbn037uk', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:516: AssertionError"}, "teardown": {"duration": 0.0001230219999115434, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006066404999955921, "outcome": "passed"}, "call": {"duration": 2.3760117479999963, "outcome": "passed"}, "teardown": {"duration": 0.00012971199998901284, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "add_product_tool"}, "setup": {"duration": 0.006198908999976993, "outcome": "passed"}, "call": {"duration": 3.890477181000051, "outcome": "passed"}, "teardown": {"duration": 0.00014344299995627807, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006371402000013404, "outcome": "passed"}, "call": {"duration": 7.734182036999982, "outcome": "passed"}, "teardown": {"duration": 0.00015583200001856312, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006361423000043942, "outcome": "passed"}, "call": {"duration": 5.774718602999997, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'getMonthlyExpenseComparison'\nassert 'getMonthlyExpenseComparison' == 'getMonthlyExpenseSummary'\n  \n  - getMonthlyExpenseSummary\n  ?                  ^^ ^  ^\n  + getMonthlyExpenseComparison\n  ?                  ^^ ^  ^^^^"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fa037d58610>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n>               assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\nE               AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'getMonthlyExpenseComparison'\nE               assert 'getMonthlyExpenseComparison' == 'getMonthlyExpenseSummary'\nE                 \nE                 - getMonthlyExpenseSummary\nE                 ?                  ^^ ^  ^\nE                 + getMonthlyExpenseComparison\nE                 ?                  ^^ ^  ^^^^\n\ntests/verifications/openai_api/test_chat_completion.py:523: AssertionError"}, "teardown": {"duration": 0.0001549830000158181, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "stream=False"}, "setup": {"duration": 0.007043914000064433, "outcome": "passed"}, "call": {"duration": 0.00011620199995832081, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama3.3:70b-instruct-q4_K_M on provider ollama based on config.')"}, "teardown": {"duration": 0.00010884199991778587, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "stream=True"}, "setup": {"duration": 0.007017714000085107, "outcome": "passed"}, "call": {"duration": 9.337100004813692e-05, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama3.3:70b-instruct-q4_K_M on provider ollama based on config.')"}, "teardown": {"duration": 0.0006235810000134734, "outcome": "passed"}}]}