{"created": 1745532129.6523387, "duration": 94.03675079345703, "exitcode": 1, "root": "/actions-runner/_work/lls-openai-client/lls-openai-client", "environment": {}, "summary": {"passed": 26, "failed": 8, "skipped": 4, "total": 38, "collected": 38}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 183}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 202}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 271}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 295}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 323}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 374}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 397}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "type": "Function", "lineno": 599}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "earth"}, "setup": {"duration": 0.03435403999992559, "outcome": "passed"}, "call": {"duration": 3.3738997349998954, "outcome": "passed"}, "teardown": {"duration": 0.00017113200010498986, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "saturn"}, "setup": {"duration": 0.007212135000145281, "outcome": "passed"}, "call": {"duration": 5.047086540999999, "outcome": "passed"}, "teardown": {"duration": 0.0001567330000398215, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "earth"}, "setup": {"duration": 0.007181814999967173, "outcome": "passed"}, "call": {"duration": 4.46404528700009, "outcome": "passed"}, "teardown": {"duration": 0.00015727200002402242, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "saturn"}, "setup": {"duration": 0.006956982000019707, "outcome": "passed"}, "call": {"duration": 2.136077908000061, "outcome": "passed"}, "teardown": {"duration": 0.00019716299993888242, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_missing"}, "setup": {"duration": 0.006863949000035063, "outcome": "passed"}, "call": {"duration": 0.009449581000126273, "outcome": "passed"}, "teardown": {"duration": 0.00011085100004493142, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006115777999866623, "outcome": "passed"}, "call": {"duration": 0.004649804000109725, "outcome": "passed"}, "teardown": {"duration": 0.0001091520000500168, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.005943845000047077, "outcome": "passed"}, "call": {"duration": 0.13559949099999358, "outcome": "passed"}, "teardown": {"duration": 0.00012241099989296345, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.006095577999985835, "outcome": "passed"}, "call": {"duration": 0.08112020899989147, "outcome": "passed"}, "teardown": {"duration": 0.00011290199995528383, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.0060318069999993895, "outcome": "passed"}, "call": {"duration": 0.10074313299992355, "outcome": "passed"}, "teardown": {"duration": 0.00013059200000498095, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_missing"}, "setup": {"duration": 0.006696986999941146, "outcome": "passed"}, "call": {"duration": 0.003244901000016398, "outcome": "passed"}, "teardown": {"duration": 0.00016003199993974704, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006797149000021818, "outcome": "passed"}, "call": {"duration": 0.0038654820000374457, "outcome": "passed"}, "teardown": {"duration": 0.00012228199989294808, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.00672491699992861, "outcome": "passed"}, "call": {"duration": 0.009990900000047986, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 179, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 58, "message": "in __stream__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 50, "message": "in _iter_events"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 280, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 291, "message": "in _iter_chunks"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 897, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 951, "message": "in iter_raw"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_client.py", "lineno": 153, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 126, "message": "in __iter__"}, {"path": "/usr/lib64/python3.11/contextlib.py", "lineno": 158, "message": "in __exit__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "RemoteProtocolError"}], "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:127: in __iter__\n    for part in self._httpcore_stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:407: in __iter__\n    raise exc from None\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:403: in __iter__\n    for part in self._stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:342: in __iter__\n    raise exc\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:334: in __iter__\n    for chunk in self._connection._receive_response_body(**kwargs):\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:203: in _receive_response_body\n    event = self._receive_event(timeout=timeout)\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:213: in _receive_event\n    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'h11._util.RemoteProtocolError'>: <class 'httpcore.RemoteProtocolError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpcore/_exceptions.py:14: RemoteProtocolError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f18c7700890>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'invalid'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>           for _chunk in response:\n\ntests/verifications/openai_api/test_chat_completion.py:179: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:58: in __stream__\n    for sse in iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:50: in _iter_events\n    yield from self._decoder.iter_bytes(self.response.iter_bytes())\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:280: in iter_bytes\n    for chunk in self._iter_chunks(iterator):\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:291: in _iter_chunks\n    for chunk in iterator:\n.venv/lib64/python3.11/site-packages/httpx/_models.py:897: in iter_bytes\n    for raw_bytes in self.iter_raw():\n.venv/lib64/python3.11/site-packages/httpx/_models.py:951: in iter_raw\n    for raw_stream_bytes in self.stream:\n.venv/lib64/python3.11/site-packages/httpx/_client.py:153: in __iter__\n    for chunk in self._stream:\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:126: in __iter__\n    with map_httpcore_exceptions():\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:118: RemoteProtocolError"}, "teardown": {"duration": 0.00013455200019052427, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.006203258999903483, "outcome": "passed"}, "call": {"duration": 0.009103184999958103, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 179, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 58, "message": "in __stream__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 50, "message": "in _iter_events"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 280, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 291, "message": "in _iter_chunks"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 897, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 951, "message": "in iter_raw"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_client.py", "lineno": 153, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 126, "message": "in __iter__"}, {"path": "/usr/lib64/python3.11/contextlib.py", "lineno": 158, "message": "in __exit__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "RemoteProtocolError"}], "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:127: in __iter__\n    for part in self._httpcore_stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:407: in __iter__\n    raise exc from None\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:403: in __iter__\n    for part in self._stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:342: in __iter__\n    raise exc\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:334: in __iter__\n    for chunk in self._connection._receive_response_body(**kwargs):\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:203: in _receive_response_body\n    event = self._receive_event(timeout=timeout)\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:213: in _receive_event\n    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'h11._util.RemoteProtocolError'>: <class 'httpcore.RemoteProtocolError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpcore/_exceptions.py:14: RemoteProtocolError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7f18c7004690>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>           for _chunk in response:\n\ntests/verifications/openai_api/test_chat_completion.py:179: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:58: in __stream__\n    for sse in iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:50: in _iter_events\n    yield from self._decoder.iter_bytes(self.response.iter_bytes())\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:280: in iter_bytes\n    for chunk in self._iter_chunks(iterator):\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:291: in _iter_chunks\n    for chunk in iterator:\n.venv/lib64/python3.11/site-packages/httpx/_models.py:897: in iter_bytes\n    for raw_bytes in self.iter_raw():\n.venv/lib64/python3.11/site-packages/httpx/_models.py:951: in iter_raw\n    for raw_stream_bytes in self.stream:\n.venv/lib64/python3.11/site-packages/httpx/_client.py:153: in __iter__\n    for chunk in self._stream:\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:126: in __iter__\n    with map_httpcore_exceptions():\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:118: RemoteProtocolError"}, "teardown": {"duration": 0.0001448929999696702, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.006385863000105019, "outcome": "passed"}, "call": {"duration": 0.009943489999841404, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 179, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 58, "message": "in __stream__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 50, "message": "in _iter_events"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 280, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 291, "message": "in _iter_chunks"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 897, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 951, "message": "in iter_raw"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_client.py", "lineno": 153, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 126, "message": "in __iter__"}, {"path": "/usr/lib64/python3.11/contextlib.py", "lineno": 158, "message": "in __exit__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "RemoteProtocolError"}], "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:127: in __iter__\n    for part in self._httpcore_stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:407: in __iter__\n    raise exc from None\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:403: in __iter__\n    for part in self._stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:342: in __iter__\n    raise exc\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:334: in __iter__\n    for chunk in self._connection._receive_response_body(**kwargs):\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:203: in _receive_response_body\n    event = self._receive_event(timeout=timeout)\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:213: in _receive_event\n    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'h11._util.RemoteProtocolError'>: <class 'httpcore.RemoteProtocolError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpcore/_exceptions.py:14: RemoteProtocolError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f18c705de90>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>           for _chunk in response:\n\ntests/verifications/openai_api/test_chat_completion.py:179: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:58: in __stream__\n    for sse in iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:50: in _iter_events\n    yield from self._decoder.iter_bytes(self.response.iter_bytes())\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:280: in iter_bytes\n    for chunk in self._iter_chunks(iterator):\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:291: in _iter_chunks\n    for chunk in iterator:\n.venv/lib64/python3.11/site-packages/httpx/_models.py:897: in iter_bytes\n    for raw_bytes in self.iter_raw():\n.venv/lib64/python3.11/site-packages/httpx/_models.py:951: in iter_raw\n    for raw_stream_bytes in self.stream:\n.venv/lib64/python3.11/site-packages/httpx/_client.py:153: in __iter__\n    for chunk in self._stream:\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:126: in __iter__\n    with map_httpcore_exceptions():\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:118: RemoteProtocolError"}, "teardown": {"duration": 0.00011563199996089679, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 183, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006034995999925741, "outcome": "passed"}, "call": {"duration": 0.00011336200009282038, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 192, 'Skipped: Skipping test_chat_non_streaming_image for model llama3.3:70b-instruct-q4_K_M on provider ollama-llama-stack based on config.')"}, "teardown": {"duration": 0.00010160199985875806, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 202, "outcome": "skipped", "keywords": ["test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006557884999892849, "outcome": "passed"}, "call": {"duration": 9.873099998003454e-05, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 211, 'Skipped: Skipping test_chat_streaming_image for model llama3.3:70b-instruct-q4_K_M on provider ollama-llama-stack based on config.')"}, "teardown": {"duration": 0.0001036720000229252, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "calendar"}, "setup": {"duration": 0.006081607999931293, "outcome": "passed"}, "call": {"duration": 1.9848796270000548, "outcome": "passed"}, "teardown": {"duration": 0.00014602300007027225, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "math"}, "setup": {"duration": 0.006366642000102729, "outcome": "passed"}, "call": {"duration": 13.35073155100008, "outcome": "passed"}, "teardown": {"duration": 0.00014045199986867374, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "calendar"}, "setup": {"duration": 0.008888741999953709, "outcome": "passed"}, "call": {"duration": 1.9019097929999589, "outcome": "passed"}, "teardown": {"duration": 0.00012292199994590192, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "math"}, "setup": {"duration": 0.006265119999852686, "outcome": "passed"}, "call": {"duration": 10.861400715999935, "outcome": "passed"}, "teardown": {"duration": 0.0001282919999994192, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 271, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006452583000054801, "outcome": "passed"}, "call": {"duration": 1.4756798640000852, "outcome": "passed"}, "teardown": {"duration": 0.00013237199982540915, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 295, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006287339999971664, "outcome": "passed"}, "call": {"duration": 1.3614124329999413, "outcome": "passed"}, "teardown": {"duration": 0.00014756199993826158, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 323, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006226098999832175, "outcome": "passed"}, "call": {"duration": 1.3466960949999702, "outcome": "passed"}, "teardown": {"duration": 0.00014225300014913955, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006276230000139549, "outcome": "passed"}, "call": {"duration": 1.3601668500000414, "outcome": "passed"}, "teardown": {"duration": 0.00019445299994913512, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 374, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006372021000061068, "outcome": "passed"}, "call": {"duration": 1.3513694200000828, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [ChatCompletionMessageToolCall(id='call_1l70l8l7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] is None\n +  where [ChatCompletionMessageToolCall(id='call_1l70l8l7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_1l70l8l7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]).tool_calls\n +    where ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_1l70l8l7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]) = Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_1l70l8l7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)])).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f18c75c8e10>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [ChatCompletionMessageToolCall(id='call_1l70l8l7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] is None\nE        +  where [ChatCompletionMessageToolCall(id='call_1l70l8l7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_1l70l8l7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]).tool_calls\nE        +    where ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_1l70l8l7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)]) = Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_1l70l8l7', function=Function(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function', index=0)])).message\n\ntests/verifications/openai_api/test_chat_completion.py:394: AssertionError"}, "teardown": {"duration": 0.00013851199992132024, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 397, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006317340000123295, "outcome": "passed"}, "call": {"duration": 1.2879160989998581, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 421, "message": "AssertionError: Expected no tool call chunks when tool_choice='none'\nassert not [ChoiceDeltaToolCall(index=0, id='call_gv3tihw5', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')]\n +  where [ChoiceDeltaToolCall(index=0, id='call_gv3tihw5', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')] = ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_gv3tihw5', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 421, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f18c6fad850>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=True,\n        )\n    \n        content = \"\"\n        for chunk in stream:\n            delta = chunk.choices[0].delta\n            if delta.content:\n                content += delta.content\n>           assert not delta.tool_calls, \"Expected no tool call chunks when tool_choice='none'\"\nE           AssertionError: Expected no tool call chunks when tool_choice='none'\nE           assert not [ChoiceDeltaToolCall(index=0, id='call_gv3tihw5', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')]\nE            +  where [ChoiceDeltaToolCall(index=0, id='call_gv3tihw5', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')] = ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_gv3tihw5', function=ChoiceDeltaToolCallFunction(arguments='{\"location\":\"San Francisco, USA\"}', name='get_weather'), type='function')]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:421: AssertionError"}, "teardown": {"duration": 0.00013608199992631853, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006206598999824564, "outcome": "passed"}, "call": {"duration": 0.949197174000119, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 512, "message": "AssertionError: Expected one of ['sol'] in content, but got: 'This task is beyond my capabilities with the given functions.'\nassert False\n +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f18c7803d80>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 512, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f18c7977910>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert assistant_message.content is not None, \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]  # This is now a list\n                content_lower = assistant_message.content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{assistant_message.content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: 'This task is beyond my capabilities with the given functions.'\nE               assert False\nE                +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f18c7803d80>)\n\ntests/verifications/openai_api/test_chat_completion.py:512: AssertionError"}, "teardown": {"duration": 0.00012251199996171636, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006203808000009303, "outcome": "passed"}, "call": {"duration": 2.424177828999973, "outcome": "passed"}, "teardown": {"duration": 0.00014043200008018175, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "add_product_tool"}, "setup": {"duration": 0.006355502000133129, "outcome": "passed"}, "call": {"duration": 3.974834179000027, "outcome": "passed"}, "teardown": {"duration": 0.00014840200014987204, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006294889999935549, "outcome": "passed"}, "call": {"duration": 7.773298608000005, "outcome": "passed"}, "teardown": {"duration": 0.00014956299992263666, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006440251999947577, "outcome": "passed"}, "call": {"duration": 4.968913545000078, "outcome": "passed"}, "teardown": {"duration": 0.00015862200007177307, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.00650687300003483, "outcome": "passed"}, "call": {"duration": 1.4832084949998716, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 566, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([{'function': {'arguments': '{\"location\":\"Rome, Italy\"}', 'name': 'get_weather'}, 'id': 'call_deypv6kf', 'type': 'function'}]))"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 566, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f18c70ca990>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n>           assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([{'function': {'arguments': '{\"location\":\"Rome, Italy\"}', 'name': 'get_weather'}, 'id': 'call_deypv6kf', 'type': 'function'}]))\n\ntests/verifications/openai_api/test_chat_completion.py:566: AssertionError"}, "teardown": {"duration": 0.00014746199985893327, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006260638999947332, "outcome": "passed"}, "call": {"duration": 2.4581848210000317, "outcome": "passed"}, "teardown": {"duration": 0.00017214300009982253, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "add_product_tool"}, "setup": {"duration": 0.006551303000151165, "outcome": "passed"}, "call": {"duration": 4.0041261799999575, "outcome": "passed"}, "teardown": {"duration": 0.0001578429998971842, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006262409999862939, "outcome": "passed"}, "call": {"duration": 7.594481987000108, "outcome": "passed"}, "teardown": {"duration": 0.00019198300014977576, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006285359999992579, "outcome": "passed"}, "call": {"duration": 5.8407923610000125, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 573, "message": "AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'compareMonthlyExpenses'\nassert 'compareMonthlyExpenses' == 'getMonthlyExpenseSummary'\n  \n  - getMonthlyExpenseSummary\n  + compareMonthlyExpenses"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 573, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f18c76eaf10>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n>               assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\nE               AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'compareMonthlyExpenses'\nE               assert 'compareMonthlyExpenses' == 'getMonthlyExpenseSummary'\nE                 \nE                 - getMonthlyExpenseSummary\nE                 + compareMonthlyExpenses\n\ntests/verifications/openai_api/test_chat_completion.py:573: AssertionError"}, "teardown": {"duration": 0.00015325300000768038, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "lineno": 599, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "stream=False"}, "setup": {"duration": 0.006792578999920806, "outcome": "passed"}, "call": {"duration": 0.00010840100003406405, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 606, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama3.3:70b-instruct-q4_K_M on provider ollama-llama-stack based on config.')"}, "teardown": {"duration": 0.00011039199989681947, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "lineno": 599, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "stream=True"}, "setup": {"duration": 0.007144364000168935, "outcome": "passed"}, "call": {"duration": 0.00011586200002966507, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 606, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama3.3:70b-instruct-q4_K_M on provider ollama-llama-stack based on config.')"}, "teardown": {"duration": 0.0005863689998477639, "outcome": "passed"}}]}