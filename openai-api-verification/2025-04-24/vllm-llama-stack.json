{"created": 1745509591.3166938, "duration": 69.86638975143433, "exitcode": 1, "root": "/actions-runner/_work/lls-openai-client/lls-openai-client", "environment": {}, "summary": {"passed": 24, "failed": 10, "skipped": 4, "total": 38, "collected": 38}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 183}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 202}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 271}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 295}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 323}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 374}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 397}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=False]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=True]", "type": "Function", "lineno": 599}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "earth"}, "setup": {"duration": 0.03389325099999496, "outcome": "passed"}, "call": {"duration": 0.5016898149999633, "outcome": "passed"}, "teardown": {"duration": 0.0001792140000134168, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "saturn"}, "setup": {"duration": 0.006946287999994638, "outcome": "passed"}, "call": {"duration": 1.8372931059999473, "outcome": "passed"}, "teardown": {"duration": 0.00013867400002709473, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "earth"}, "setup": {"duration": 0.0066881720000537825, "outcome": "passed"}, "call": {"duration": 0.4838202360000423, "outcome": "passed"}, "teardown": {"duration": 0.00017395400004716066, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "saturn"}, "setup": {"duration": 0.007059981999987031, "outcome": "passed"}, "call": {"duration": 1.1129400069999065, "outcome": "passed"}, "teardown": {"duration": 0.00017195399993852334, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "messages_missing"}, "setup": {"duration": 0.0063454730000103154, "outcome": "passed"}, "call": {"duration": 0.004310150000037538, "outcome": "passed"}, "teardown": {"duration": 0.00029510700005630497, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006029925000007097, "outcome": "passed"}, "call": {"duration": 0.003811397999925248, "outcome": "passed"}, "teardown": {"duration": 0.0001101819999576037, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006016215000045122, "outcome": "passed"}, "call": {"duration": 0.13007824299995718, "outcome": "passed"}, "teardown": {"duration": 0.00015781400009018398, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.006721072999994249, "outcome": "passed"}, "call": {"duration": 0.07584758999996666, "outcome": "passed"}, "teardown": {"duration": 0.00011591399993449159, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.006593998999960604, "outcome": "passed"}, "call": {"duration": 0.20576442900005532, "outcome": "passed"}, "teardown": {"duration": 0.00015840399998978683, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "messages_missing"}, "setup": {"duration": 0.007073662000038894, "outcome": "passed"}, "call": {"duration": 0.36501799199993457, "outcome": "passed"}, "teardown": {"duration": 0.000172664000047007, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006732181999950626, "outcome": "passed"}, "call": {"duration": 0.0037503659999629235, "outcome": "passed"}, "teardown": {"duration": 0.0001108129999920493, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006646971000009216, "outcome": "passed"}, "call": {"duration": 0.007399459999987812, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 179, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 58, "message": "in __stream__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 50, "message": "in _iter_events"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 280, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 291, "message": "in _iter_chunks"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 897, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 951, "message": "in iter_raw"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_client.py", "lineno": 153, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 126, "message": "in __iter__"}, {"path": "/usr/lib64/python3.11/contextlib.py", "lineno": 158, "message": "in __exit__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "RemoteProtocolError"}], "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:127: in __iter__\n    for part in self._httpcore_stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:407: in __iter__\n    raise exc from None\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:403: in __iter__\n    for part in self._stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:342: in __iter__\n    raise exc\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:334: in __iter__\n    for chunk in self._connection._receive_response_body(**kwargs):\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:203: in _receive_response_body\n    event = self._receive_event(timeout=timeout)\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:213: in _receive_event\n    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'h11._util.RemoteProtocolError'>: <class 'httpcore.RemoteProtocolError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpcore/_exceptions.py:14: RemoteProtocolError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fca9401f3d0>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'invalid'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>           for _chunk in response:\n\ntests/verifications/openai_api/test_chat_completion.py:179: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:58: in __stream__\n    for sse in iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:50: in _iter_events\n    yield from self._decoder.iter_bytes(self.response.iter_bytes())\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:280: in iter_bytes\n    for chunk in self._iter_chunks(iterator):\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:291: in _iter_chunks\n    for chunk in iterator:\n.venv/lib64/python3.11/site-packages/httpx/_models.py:897: in iter_bytes\n    for raw_bytes in self.iter_raw():\n.venv/lib64/python3.11/site-packages/httpx/_models.py:951: in iter_raw\n    for raw_stream_bytes in self.stream:\n.venv/lib64/python3.11/site-packages/httpx/_client.py:153: in __iter__\n    for chunk in self._stream:\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:126: in __iter__\n    with map_httpcore_exceptions():\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:118: RemoteProtocolError"}, "teardown": {"duration": 0.00013939299992671295, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.006183199000020068, "outcome": "passed"}, "call": {"duration": 0.00690742799997679, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 179, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 58, "message": "in __stream__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 50, "message": "in _iter_events"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 280, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 291, "message": "in _iter_chunks"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 897, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 951, "message": "in iter_raw"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_client.py", "lineno": 153, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 126, "message": "in __iter__"}, {"path": "/usr/lib64/python3.11/contextlib.py", "lineno": 158, "message": "in __exit__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "RemoteProtocolError"}], "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:127: in __iter__\n    for part in self._httpcore_stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:407: in __iter__\n    raise exc from None\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:403: in __iter__\n    for part in self._stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:342: in __iter__\n    raise exc\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:334: in __iter__\n    for chunk in self._connection._receive_response_body(**kwargs):\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:203: in _receive_response_body\n    event = self._receive_event(timeout=timeout)\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:213: in _receive_event\n    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'h11._util.RemoteProtocolError'>: <class 'httpcore.RemoteProtocolError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpcore/_exceptions.py:14: RemoteProtocolError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7fca9372c3d0>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>           for _chunk in response:\n\ntests/verifications/openai_api/test_chat_completion.py:179: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:58: in __stream__\n    for sse in iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:50: in _iter_events\n    yield from self._decoder.iter_bytes(self.response.iter_bytes())\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:280: in iter_bytes\n    for chunk in self._iter_chunks(iterator):\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:291: in _iter_chunks\n    for chunk in iterator:\n.venv/lib64/python3.11/site-packages/httpx/_models.py:897: in iter_bytes\n    for raw_bytes in self.iter_raw():\n.venv/lib64/python3.11/site-packages/httpx/_models.py:951: in iter_raw\n    for raw_stream_bytes in self.stream:\n.venv/lib64/python3.11/site-packages/httpx/_client.py:153: in __iter__\n    for chunk in self._stream:\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:126: in __iter__\n    with map_httpcore_exceptions():\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:118: RemoteProtocolError"}, "teardown": {"duration": 0.00014884299991990702, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.00626724199992168, "outcome": "passed"}, "call": {"duration": 0.007725298999957886, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 179, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 58, "message": "in __stream__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 50, "message": "in _iter_events"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 280, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 291, "message": "in _iter_chunks"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 897, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 951, "message": "in iter_raw"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_client.py", "lineno": 153, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 126, "message": "in __iter__"}, {"path": "/usr/lib64/python3.11/contextlib.py", "lineno": 158, "message": "in __exit__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "RemoteProtocolError"}], "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:127: in __iter__\n    for part in self._httpcore_stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:407: in __iter__\n    raise exc from None\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:403: in __iter__\n    for part in self._stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:342: in __iter__\n    raise exc\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:334: in __iter__\n    for chunk in self._connection._receive_response_body(**kwargs):\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:203: in _receive_response_body\n    event = self._receive_event(timeout=timeout)\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:213: in _receive_event\n    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'h11._util.RemoteProtocolError'>: <class 'httpcore.RemoteProtocolError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpcore/_exceptions.py:14: RemoteProtocolError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7fca93ae24d0>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>           for _chunk in response:\n\ntests/verifications/openai_api/test_chat_completion.py:179: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:58: in __stream__\n    for sse in iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:50: in _iter_events\n    yield from self._decoder.iter_bytes(self.response.iter_bytes())\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:280: in iter_bytes\n    for chunk in self._iter_chunks(iterator):\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:291: in _iter_chunks\n    for chunk in iterator:\n.venv/lib64/python3.11/site-packages/httpx/_models.py:897: in iter_bytes\n    for raw_bytes in self.iter_raw():\n.venv/lib64/python3.11/site-packages/httpx/_models.py:951: in iter_raw\n    for raw_stream_bytes in self.stream:\n.venv/lib64/python3.11/site-packages/httpx/_client.py:153: in __iter__\n    for chunk in self._stream:\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:126: in __iter__\n    with map_httpcore_exceptions():\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:118: RemoteProtocolError"}, "teardown": {"duration": 0.00011553299998467992, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 183, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006015313999910177, "outcome": "passed"}, "call": {"duration": 0.0001029320000043299, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 192, 'Skipped: Skipping test_chat_non_streaming_image for model RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16 on provider vllm-llama-stack based on config.')"}, "teardown": {"duration": 0.00010373199995683535, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 202, "outcome": "skipped", "keywords": ["test_chat_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006390795000015714, "outcome": "passed"}, "call": {"duration": 9.74829999904614e-05, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 211, 'Skipped: Skipping test_chat_streaming_image for model RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16 on provider vllm-llama-stack based on config.')"}, "teardown": {"duration": 0.00010144299994863104, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "calendar"}, "setup": {"duration": 0.006001274000027479, "outcome": "passed"}, "call": {"duration": 1.852643090000015, "outcome": "passed"}, "teardown": {"duration": 0.00012212299998282106, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "math"}, "setup": {"duration": 0.006197198999984721, "outcome": "passed"}, "call": {"duration": 11.681501696000055, "outcome": "passed"}, "teardown": {"duration": 0.00013744300008511345, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "calendar"}, "setup": {"duration": 0.006512598000085745, "outcome": "passed"}, "call": {"duration": 1.690208261999942, "outcome": "passed"}, "teardown": {"duration": 0.00014860299995689275, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "math"}, "setup": {"duration": 0.006317202999980509, "outcome": "passed"}, "call": {"duration": 14.283727902999999, "outcome": "passed"}, "teardown": {"duration": 0.00014434399997753644, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 271, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.0064657679999982065, "outcome": "passed"}, "call": {"duration": 1.5138461080000525, "outcome": "passed"}, "teardown": {"duration": 0.0001260029999912149, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 295, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006312123000043357, "outcome": "passed"}, "call": {"duration": 1.5218283350000092, "outcome": "passed"}, "teardown": {"duration": 0.0001258029999462451, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 323, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006168298999909894, "outcome": "passed"}, "call": {"duration": 1.289847047999956, "outcome": "passed"}, "teardown": {"duration": 0.0001517440000498027, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 347, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006186400000046888, "outcome": "passed"}, "call": {"duration": 1.2882170960000394, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 368, "message": "AssertionError: Expected tool call when tool_choice='required'\nassert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 368, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fca93c918d0>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n        _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n    \n>       assert len(tool_calls_buffer) > 0, \"Expected tool call when tool_choice='required'\"\nE       AssertionError: Expected tool call when tool_choice='required'\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/verifications/openai_api/test_chat_completion.py:368: AssertionError"}, "teardown": {"duration": 0.00013595299992630316, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 374, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006232830999920225, "outcome": "passed"}, "call": {"duration": 1.0467428140000266, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [] is None\n +  where [] = ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\n +    where ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=128008).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]>>\nopenai_client = <openai.OpenAI object at 0x7fca93f96d90>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [] is None\nE        +  where [] = ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\nE        +    where ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=128008).message\n\ntests/verifications/openai_api/test_chat_completion.py:394: AssertionError"}, "teardown": {"duration": 0.00014012300005106226, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 397, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006268502000011722, "outcome": "passed"}, "call": {"duration": 1.0575200719999884, "outcome": "passed"}, "teardown": {"duration": 0.00011994299995876645, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006260182000005443, "outcome": "passed"}, "call": {"duration": 1.5143455720000247, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='chatcmpl-tool-a5f2ecef3eee4ed3810fc448d8bb6333', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]))\n +    where [ChatCompletionMessageToolCall(id='chatcmpl-tool-a5f2ecef3eee4ed3810fc448d8bb6333', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-a5f2ecef3eee4ed3810fc448d8bb6333', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')], reasoning_content=None).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 484, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fca93eee950>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='chatcmpl-tool-a5f2ecef3eee4ed3810fc448d8bb6333', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')]))\nE            +    where [ChatCompletionMessageToolCall(id='chatcmpl-tool-a5f2ecef3eee4ed3810fc448d8bb6333', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')] = ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='chatcmpl-tool-a5f2ecef3eee4ed3810fc448d8bb6333', function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_weather'), type='function')], reasoning_content=None).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:484: AssertionError"}, "teardown": {"duration": 0.00014142300005914876, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006283101999997598, "outcome": "passed"}, "call": {"duration": 3.541957835000062, "outcome": "passed"}, "teardown": {"duration": 0.00015129500002331042, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "add_product_tool"}, "setup": {"duration": 0.006337495000025228, "outcome": "passed"}, "call": {"duration": 3.19926120599996, "outcome": "passed"}, "teardown": {"duration": 0.0001303330000155256, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.00625440199996774, "outcome": "passed"}, "call": {"duration": 2.7686862479999945, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'detail': \"Error code: 400 - {'object': 'error', 'message': 'This model only supports single tool-calls at once!', 'type': 'BadRequestError', 'param': None, 'code': 400}\"}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 466, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fca93fcae10>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n>           response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:466: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7fca93fcae10>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'detail': \"Error code: 400 - {'object': 'error', 'message': 'This model only supports single tool-calls at once!', 'type': 'BadRequestError', 'param': None, 'code': 400}\"}\n\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00017176399990148639, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006303384000034384, "outcome": "passed"}, "call": {"duration": 1.5155047190000914, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 495, "message": "AssertionError: Expected arguments '{'month': 1, 'year': 2025}', got '{'month': '1', 'year': '2025'}'\nassert {'month': '1', 'year': '2025'} == {'month': 1, 'year': 2025}\n  \n  Differing items:\n  {'year': '2025'} != {'year': 2025}\n  {'month': '1'} != {'month': 1}\n  \n  Full diff:\n    {\n  -     'month': 1,\n  +     'month': '1',\n  ?              + +\n  -     'year': 2025,\n  +     'year': '2025',\n  ?             +    +\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 495, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fca93cf0ad0>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'month': 1, 'year': 2025}', got '{'month': '1', 'year': '2025'}'\nE               assert {'month': '1', 'year': '2025'} == {'month': 1, 'year': 2025}\nE                 \nE                 Differing items:\nE                 {'year': '2025'} != {'year': 2025}\nE                 {'month': '1'} != {'month': 1}\nE                 \nE                 Full diff:\nE                   {\nE                 -     'month': 1,\nE                 +     'month': '1',\nE                 ?              + +\nE                 -     'year': 2025,\nE                 +     'year': '2025',\nE                 ?             +    +\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:495: AssertionError"}, "teardown": {"duration": 0.00014139400002477487, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006352283000069292, "outcome": "passed"}, "call": {"duration": 0.7756937530000414, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 595, "message": "AssertionError: Expected one of ['sol'] in content, but got: 'The provided functions are insufficient for me to answer this question.'\nassert False\n +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fca93fc4900>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 595, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fca93c38950>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert accumulated_content is not None and accumulated_content != \"\", \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]\n                content_lower = accumulated_content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{accumulated_content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: 'The provided functions are insufficient for me to answer this question.'\nE               assert False\nE                +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7fca93fc4900>)\n\ntests/verifications/openai_api/test_chat_completion.py:595: AssertionError"}, "teardown": {"duration": 0.00014521299999614712, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.00623776000009002, "outcome": "passed"}, "call": {"duration": 2.3552966140000535, "outcome": "passed"}, "teardown": {"duration": 0.00015870400000039808, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "add_product_tool"}, "setup": {"duration": 0.00643029499997283, "outcome": "passed"}, "call": {"duration": 3.2201912839999522, "outcome": "passed"}, "teardown": {"duration": 0.00015700400001605885, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.0063214639999387146, "outcome": "passed"}, "call": {"duration": 6.535241327999984, "outcome": "passed"}, "teardown": {"duration": 0.00019536499996775092, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006366074999959892, "outcome": "passed"}, "call": {"duration": 1.5251016780000555, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 578, "message": "AssertionError: Expected arguments '{'month': 1, 'year': 2025}', got '{'month': '1', 'year': '2025'}'\nassert {'month': '1', 'year': '2025'} == {'month': 1, 'year': 2025}\n  \n  Differing items:\n  {'year': '2025'} != {'year': 2025}\n  {'month': '1'} != {'month': 1}\n  \n  Full diff:\n    {\n  -     'month': 1,\n  +     'month': '1',\n  ?              + +\n  -     'year': 2025,\n  +     'year': '2025',\n  ?             +    +\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 578, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7fca9405bc50>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'month': 1, 'year': 2025}', got '{'month': '1', 'year': '2025'}'\nE               assert {'month': '1', 'year': '2025'} == {'month': 1, 'year': 2025}\nE                 \nE                 Differing items:\nE                 {'year': '2025'} != {'year': 2025}\nE                 {'month': '1'} != {'month': 1}\nE                 \nE                 Full diff:\nE                   {\nE                 -     'month': 1,\nE                 +     'month': '1',\nE                 ?              + +\nE                 -     'year': 2025,\nE                 +     'year': '2025',\nE                 ?             +    +\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:578: AssertionError"}, "teardown": {"duration": 0.00013438400003451534, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=False]", "lineno": 599, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=False]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "stream=False"}, "setup": {"duration": 0.006829206000020349, "outcome": "passed"}, "call": {"duration": 0.00011168300000008458, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 606, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16 on provider vllm-llama-stack based on config.')"}, "teardown": {"duration": 0.00010654300001533556, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=True]", "lineno": 599, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=True]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "stream=True"}, "setup": {"duration": 0.007078243000023576, "outcome": "passed"}, "call": {"duration": 9.612300004846475e-05, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 606, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16 on provider vllm-llama-stack based on config.')"}, "teardown": {"duration": 0.0005864150000434165, "outcome": "passed"}}]}