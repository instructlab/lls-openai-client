{"created": 1745505225.886133, "duration": 82.60999512672424, "exitcode": 1, "root": "/actions-runner/_work/lls-openai-client/lls-openai-client", "environment": {}, "summary": {"passed": 24, "failed": 10, "skipped": 4, "total": 38, "collected": 38}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "type": "Function", "lineno": 96}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "type": "Function", "lineno": 115}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "type": "Function", "lineno": 139}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "type": "Function", "lineno": 160}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 183}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 202}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "type": "Function", "lineno": 226}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "type": "Function", "lineno": 249}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 271}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 295}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 323}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 374}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "type": "Function", "lineno": 397}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "type": "Function", "lineno": 425}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "type": "Function", "lineno": 516}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=False]", "type": "Function", "lineno": 599}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=True]", "type": "Function", "lineno": 599}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "earth"}, "setup": {"duration": 0.03250779400002557, "outcome": "passed"}, "call": {"duration": 1.067124232000026, "outcome": "passed"}, "teardown": {"duration": 0.00016614399999070883, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "lineno": 96, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "saturn"}, "setup": {"duration": 0.006976245999965158, "outcome": "passed"}, "call": {"duration": 1.730842989999985, "outcome": "passed"}, "teardown": {"duration": 0.0001345830000332171, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "earth"}, "setup": {"duration": 0.006747169999982816, "outcome": "passed"}, "call": {"duration": 0.48661215999999285, "outcome": "passed"}, "teardown": {"duration": 0.00016890299997385227, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "lineno": 115, "outcome": "passed", "keywords": ["test_chat_streaming_basic[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "saturn"}, "setup": {"duration": 0.006738739000013538, "outcome": "passed"}, "call": {"duration": 1.9670013680000125, "outcome": "passed"}, "teardown": {"duration": 0.0001960639999651903, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "messages_missing"}, "setup": {"duration": 0.007066215999998349, "outcome": "passed"}, "call": {"duration": 0.004806899000016074, "outcome": "passed"}, "teardown": {"duration": 0.00012406199999759338, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006649876999972548, "outcome": "passed"}, "call": {"duration": 0.004248987999972087, "outcome": "passed"}, "teardown": {"duration": 0.00013338200000134748, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006615396999961831, "outcome": "passed"}, "call": {"duration": 0.13146058000000949, "outcome": "passed"}, "teardown": {"duration": 0.00014646299996456946, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.006625546999998733, "outcome": "passed"}, "call": {"duration": 0.07542489000002206, "outcome": "passed"}, "teardown": {"duration": 0.00011631299997816313, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "lineno": 139, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.006568966000031651, "outcome": "passed"}, "call": {"duration": 0.20703434400002152, "outcome": "passed"}, "teardown": {"duration": 0.00015447300000914765, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "messages_missing"}, "setup": {"duration": 0.0070534760000100505, "outcome": "passed"}, "call": {"duration": 0.3540378769999961, "outcome": "passed"}, "teardown": {"duration": 0.00011621200002309706, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "lineno": 160, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006714118999980201, "outcome": "passed"}, "call": {"duration": 0.003875849999985803, "outcome": "passed"}, "teardown": {"duration": 0.00013260299999728886, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.01368160300000909, "outcome": "passed"}, "call": {"duration": 0.007642679000014141, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 179, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 58, "message": "in __stream__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 50, "message": "in _iter_events"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 280, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 291, "message": "in _iter_chunks"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 897, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 951, "message": "in iter_raw"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_client.py", "lineno": 153, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 126, "message": "in __iter__"}, {"path": "/usr/lib64/python3.11/contextlib.py", "lineno": 158, "message": "in __exit__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "RemoteProtocolError"}], "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:127: in __iter__\n    for part in self._httpcore_stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:407: in __iter__\n    raise exc from None\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:403: in __iter__\n    for part in self._stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:342: in __iter__\n    raise exc\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:334: in __iter__\n    for chunk in self._connection._receive_response_body(**kwargs):\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:203: in _receive_response_body\n    event = self._receive_event(timeout=timeout)\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:213: in _receive_event\n    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'h11._util.RemoteProtocolError'>: <class 'httpcore.RemoteProtocolError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpcore/_exceptions.py:14: RemoteProtocolError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f4b5929aa90>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'invalid'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>           for _chunk in response:\n\ntests/verifications/openai_api/test_chat_completion.py:179: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:58: in __stream__\n    for sse in iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:50: in _iter_events\n    yield from self._decoder.iter_bytes(self.response.iter_bytes())\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:280: in iter_bytes\n    for chunk in self._iter_chunks(iterator):\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:291: in _iter_chunks\n    for chunk in iterator:\n.venv/lib64/python3.11/site-packages/httpx/_models.py:897: in iter_bytes\n    for raw_bytes in self.iter_raw():\n.venv/lib64/python3.11/site-packages/httpx/_models.py:951: in iter_raw\n    for raw_stream_bytes in self.stream:\n.venv/lib64/python3.11/site-packages/httpx/_client.py:153: in __iter__\n    for chunk in self._stream:\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:126: in __iter__\n    with map_httpcore_exceptions():\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:118: RemoteProtocolError"}, "teardown": {"duration": 0.00013615300002811637, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.0061883079999915935, "outcome": "passed"}, "call": {"duration": 0.006787330999998176, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 179, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 58, "message": "in __stream__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 50, "message": "in _iter_events"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 280, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 291, "message": "in _iter_chunks"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 897, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 951, "message": "in iter_raw"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_client.py", "lineno": 153, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 126, "message": "in __iter__"}, {"path": "/usr/lib64/python3.11/contextlib.py", "lineno": 158, "message": "in __exit__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "RemoteProtocolError"}], "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:127: in __iter__\n    for part in self._httpcore_stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:407: in __iter__\n    raise exc from None\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:403: in __iter__\n    for part in self._stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:342: in __iter__\n    raise exc\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:334: in __iter__\n    for chunk in self._connection._receive_response_body(**kwargs):\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:203: in _receive_response_body\n    event = self._receive_event(timeout=timeout)\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:213: in _receive_event\n    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'h11._util.RemoteProtocolError'>: <class 'httpcore.RemoteProtocolError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpcore/_exceptions.py:14: RemoteProtocolError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tool_choice_no_tools]>>\nopenai_client = <openai.OpenAI object at 0x7f4b58a7c2d0>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tool_choice_no_tools', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tool_choice': 'required'}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>           for _chunk in response:\n\ntests/verifications/openai_api/test_chat_completion.py:179: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:58: in __stream__\n    for sse in iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:50: in _iter_events\n    yield from self._decoder.iter_bytes(self.response.iter_bytes())\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:280: in iter_bytes\n    for chunk in self._iter_chunks(iterator):\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:291: in _iter_chunks\n    for chunk in iterator:\n.venv/lib64/python3.11/site-packages/httpx/_models.py:897: in iter_bytes\n    for raw_bytes in self.iter_raw():\n.venv/lib64/python3.11/site-packages/httpx/_models.py:951: in iter_raw\n    for raw_stream_bytes in self.stream:\n.venv/lib64/python3.11/site-packages/httpx/_client.py:153: in __iter__\n    for chunk in self._stream:\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:126: in __iter__\n    with map_httpcore_exceptions():\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:118: RemoteProtocolError"}, "teardown": {"duration": 0.00015119300002197633, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "lineno": 160, "outcome": "failed", "keywords": ["test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.0062777500000379405, "outcome": "passed"}, "call": {"duration": 0.008090097999968293, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 179, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 46, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 58, "message": "in __stream__"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 50, "message": "in _iter_events"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 280, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_streaming.py", "lineno": 291, "message": "in _iter_chunks"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 897, "message": "in iter_bytes"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_models.py", "lineno": 951, "message": "in iter_raw"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_client.py", "lineno": 153, "message": "in __iter__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 126, "message": "in __iter__"}, {"path": "/usr/lib64/python3.11/contextlib.py", "lineno": 158, "message": "in __exit__"}, {"path": ".venv/lib64/python3.11/site-packages/httpx/_transports/default.py", "lineno": 118, "message": "RemoteProtocolError"}], "longrepr": "@contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n>           yield\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:101: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:127: in __iter__\n    for part in self._httpcore_stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:407: in __iter__\n    raise exc from None\n.venv/lib64/python3.11/site-packages/httpcore/_sync/connection_pool.py:403: in __iter__\n    for part in self._stream:\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:342: in __iter__\n    raise exc\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:334: in __iter__\n    for chunk in self._connection._receive_response_body(**kwargs):\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:203: in _receive_response_body\n    event = self._receive_event(timeout=timeout)\n.venv/lib64/python3.11/site-packages/httpcore/_sync/http11.py:213: in _receive_event\n    with map_exceptions({h11.RemoteProtocolError: RemoteProtocolError}):\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nmap = {<class 'h11._util.RemoteProtocolError'>: <class 'httpcore.RemoteProtocolError'>}\n\n    @contextlib.contextmanager\n    def map_exceptions(map: ExceptionMapping) -> typing.Iterator[None]:\n        try:\n            yield\n        except Exception as exc:  # noqa: PIE786\n            for from_exc, to_exc in map.items():\n                if isinstance(exc, from_exc):\n>                   raise to_exc(exc) from exc\nE                   httpcore.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpcore/_exceptions.py:14: RemoteProtocolError\n\nThe above exception was the direct cause of the following exception:\n\nrequest = <FixtureRequest for <Function test_chat_streaming_error_handling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-tools_type_invalid]>>\nopenai_client = <openai.OpenAI object at 0x7f4b5892e650>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'tools_type_invalid', 'input': {'messages': [{'content': 'Which planet do humans live on?', 'role': 'user'}], 'tools': [{'type': 'invalid'}]}, 'output': {'error': {'status_code': 400}}}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_chat_input_validation\"][\"test_params\"][\"case\"],\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_error_handling(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        with pytest.raises(APIError) as e:\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=case[\"input\"][\"messages\"],\n                stream=True,\n                tool_choice=case[\"input\"][\"tool_choice\"] if \"tool_choice\" in case[\"input\"] else None,\n                tools=case[\"input\"][\"tools\"] if \"tools\" in case[\"input\"] else None,\n            )\n>           for _chunk in response:\n\ntests/verifications/openai_api/test_chat_completion.py:179: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_streaming.py:46: in __iter__\n    for item in self._iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:58: in __stream__\n    for sse in iterator:\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:50: in _iter_events\n    yield from self._decoder.iter_bytes(self.response.iter_bytes())\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:280: in iter_bytes\n    for chunk in self._iter_chunks(iterator):\n.venv/lib64/python3.11/site-packages/openai/_streaming.py:291: in _iter_chunks\n    for chunk in iterator:\n.venv/lib64/python3.11/site-packages/httpx/_models.py:897: in iter_bytes\n    for raw_bytes in self.iter_raw():\n.venv/lib64/python3.11/site-packages/httpx/_models.py:951: in iter_raw\n    for raw_stream_bytes in self.stream:\n.venv/lib64/python3.11/site-packages/httpx/_client.py:153: in __iter__\n    for chunk in self._stream:\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:126: in __iter__\n    with map_httpcore_exceptions():\n/usr/lib64/python3.11/contextlib.py:158: in __exit__\n    self.gen.throw(typ, value, traceback)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\n    @contextlib.contextmanager\n    def map_httpcore_exceptions() -> typing.Iterator[None]:\n        global HTTPCORE_EXC_MAP\n        if len(HTTPCORE_EXC_MAP) == 0:\n            HTTPCORE_EXC_MAP = _load_httpcore_exceptions()\n        try:\n            yield\n        except Exception as exc:\n            mapped_exc = None\n    \n            for from_exc, to_exc in HTTPCORE_EXC_MAP.items():\n                if not isinstance(exc, from_exc):\n                    continue\n                # We want to map to the most specific exception we can find.\n                # Eg if `exc` is an `httpcore.ReadTimeout`, we want to map to\n                # `httpx.ReadTimeout`, not just `httpx.TimeoutException`.\n                if mapped_exc is None or issubclass(to_exc, mapped_exc):\n                    mapped_exc = to_exc\n    \n            if mapped_exc is None:  # pragma: no cover\n                raise\n    \n            message = str(exc)\n>           raise mapped_exc(message) from exc\nE           httpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\n\n.venv/lib64/python3.11/site-packages/httpx/_transports/default.py:118: RemoteProtocolError"}, "teardown": {"duration": 0.00011398200001622172, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 183, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.005989084000020739, "outcome": "passed"}, "call": {"duration": 0.0001053220000244437, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 192, 'Skipped: Skipping test_chat_non_streaming_image for model RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16 on provider vllm-llama-stack based on config.')"}, "teardown": {"duration": 0.00010395199996082738, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 202, "outcome": "skipped", "keywords": ["test_chat_streaming_image[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006407792999993944, "outcome": "passed"}, "call": {"duration": 0.00010132200003454273, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 211, 'Skipped: Skipping test_chat_streaming_image for model RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16 on provider vllm-llama-stack based on config.')"}, "teardown": {"duration": 0.00010527199998477954, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "calendar"}, "setup": {"duration": 0.006050684999991063, "outcome": "passed"}, "call": {"duration": 5.20353879999999, "outcome": "passed"}, "teardown": {"duration": 0.00014716300000827687, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "lineno": 226, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "math"}, "setup": {"duration": 0.006342822000021897, "outcome": "passed"}, "call": {"duration": 10.666846476999979, "outcome": "passed"}, "teardown": {"duration": 0.00014494300000933436, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "calendar"}, "setup": {"duration": 0.006543114000010064, "outcome": "passed"}, "call": {"duration": 1.6945133570000053, "outcome": "passed"}, "teardown": {"duration": 0.00012531300001228374, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "lineno": 249, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "math"}, "setup": {"duration": 0.00630139899999449, "outcome": "passed"}, "call": {"duration": 17.42901846299992, "outcome": "passed"}, "teardown": {"duration": 0.00014061299998502363, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 271, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006378928000003725, "outcome": "passed"}, "call": {"duration": 1.6714048500000445, "outcome": "passed"}, "teardown": {"duration": 0.00013450300002659787, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 295, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006275545999983478, "outcome": "passed"}, "call": {"duration": 1.5250110430000632, "outcome": "passed"}, "teardown": {"duration": 0.00015416299993376015, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 323, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006148423000013281, "outcome": "passed"}, "call": {"duration": 1.3508759349999764, "outcome": "passed"}, "teardown": {"duration": 0.00014069299993479945, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 347, "outcome": "failed", "keywords": ["test_chat_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006236075000060737, "outcome": "passed"}, "call": {"duration": 1.2902304440000307, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 368, "message": "AssertionError: Expected tool call when tool_choice='required'\nassert 0 > 0\n +  where 0 = len([])"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 368, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_tool_choice_required[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f4b59032710>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_tool_choice_required(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        stream = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"required\",  # Force tool call\n            stream=True,\n        )\n    \n        _, tool_calls_buffer = _accumulate_streaming_tool_calls(stream)\n    \n>       assert len(tool_calls_buffer) > 0, \"Expected tool call when tool_choice='required'\"\nE       AssertionError: Expected tool call when tool_choice='required'\nE       assert 0 > 0\nE        +  where 0 = len([])\n\ntests/verifications/openai_api/test_chat_completion.py:368: AssertionError"}, "teardown": {"duration": 0.00013713299995288253, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 374, "outcome": "failed", "keywords": ["test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.0062720460000491585, "outcome": "passed"}, "call": {"duration": 1.0539779220000582, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError: Expected no tool calls when tool_choice='none'\nassert [] is None\n +  where [] = ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\n +    where ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=128008).message"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 394, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]>>\nopenai_client = <openai.OpenAI object at 0x7f4b592cb910>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'input': {'messages': [{'content': 'You are a helpful assistant that can use tools to get information.', 'role': 'sys..., 'properties': {...}, 'required': [...], 'type': 'object'}}, 'type': 'function'}]}, 'output': 'get_weather_tool_call'}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases[\"test_tool_calling\"][\"test_params\"][\"case\"],  # Reusing existing case for now\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_tool_choice_none(request, openai_client, model, provider, verification_config, case):\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        response = openai_client.chat.completions.create(\n            model=model,\n            messages=case[\"input\"][\"messages\"],\n            tools=case[\"input\"][\"tools\"],\n            tool_choice=\"none\",\n            stream=False,\n        )\n    \n        assert response.choices[0].message.role == \"assistant\"\n>       assert response.choices[0].message.tool_calls is None, \"Expected no tool calls when tool_choice='none'\"\nE       AssertionError: Expected no tool calls when tool_choice='none'\nE       assert [] is None\nE        +  where [] = ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None).tool_calls\nE        +    where ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None) = Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='To get the current weather conditions in San Francisco, I can search for that information.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=128008).message\n\ntests/verifications/openai_api/test_chat_completion.py:394: AssertionError"}, "teardown": {"duration": 0.00012803300000996387, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "lineno": 397, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "case0"}, "setup": {"duration": 0.006078870999999708, "outcome": "passed"}, "call": {"duration": 1.0553771319999896, "outcome": "passed"}, "teardown": {"duration": 0.00012060200003816135, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006118072000049324, "outcome": "passed"}, "call": {"duration": 0.8745409610000934, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 512, "message": "AssertionError: Expected one of ['sol'] in content, but got: 'The provided functions are insufficient for me to answer this question.'\nassert False\n +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f4b59230f20>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 512, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4b58c4d210>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert assistant_message.content is not None, \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]  # This is now a list\n                content_lower = assistant_message.content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{assistant_message.content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: 'The provided functions are insufficient for me to answer this question.'\nE               assert False\nE                +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f4b59230f20>)\n\ntests/verifications/openai_api/test_chat_completion.py:512: AssertionError"}, "teardown": {"duration": 0.0001613529999531238, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006217195000090214, "outcome": "passed"}, "call": {"duration": 3.6707288720000406, "outcome": "passed"}, "teardown": {"duration": 0.0001335220000555637, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "lineno": 425, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "add_product_tool"}, "setup": {"duration": 0.006909417999963807, "outcome": "passed"}, "call": {"duration": 3.4152252850000195, "outcome": "passed"}, "teardown": {"duration": 0.00014424299990878353, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006299737000063033, "outcome": "passed"}, "call": {"duration": 3.212513902000069, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/.venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "openai.BadRequestError: Error code: 400 - {'detail': \"Error code: 400 - {'object': 'error', 'message': 'This model only supports single tool-calls at once!', 'type': 'BadRequestError', 'param': None, 'code': 400}\"}"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 466, "message": ""}, {"path": ".venv/lib64/python3.11/site-packages/openai/_utils/_utils.py", "lineno": 279, "message": "in wrapper"}, {"path": ".venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py", "lineno": 914, "message": "in create"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1242, "message": "in post"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 919, "message": "in request"}, {"path": ".venv/lib64/python3.11/site-packages/openai/_base_client.py", "lineno": 1023, "message": "BadRequestError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4b59398cd0>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n>           response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n\ntests/verifications/openai_api/test_chat_completion.py:466: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib64/python3.11/site-packages/openai/_utils/_utils.py:279: in wrapper\n    return func(*args, **kwargs)\n.venv/lib64/python3.11/site-packages/openai/resources/chat/completions/completions.py:914: in create\n    return self._post(\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1242: in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:919: in request\n    return self._request(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <openai.OpenAI object at 0x7f4b59398cd0>\n\n    def _request(\n        self,\n        *,\n        cast_to: Type[ResponseT],\n        options: FinalRequestOptions,\n        retries_taken: int,\n        stream: bool,\n        stream_cls: type[_StreamT] | None,\n    ) -> ResponseT | _StreamT:\n        # create a copy of the options we were given so that if the\n        # options are mutated later & we then retry, the retries are\n        # given the original options\n        input_options = model_copy(options)\n    \n        cast_to = self._maybe_override_cast_to(cast_to, options)\n        options = self._prepare_options(options)\n    \n        remaining_retries = options.get_max_retries(self.max_retries) - retries_taken\n        request = self._build_request(options, retries_taken=retries_taken)\n        self._prepare_request(request)\n    \n        kwargs: HttpxSendArgs = {}\n        if self.custom_auth is not None:\n            kwargs[\"auth\"] = self.custom_auth\n    \n        log.debug(\"Sending HTTP Request: %s %s\", request.method, request.url)\n    \n        try:\n            response = self._client.send(\n                request,\n                stream=stream or self._should_stream_response_body(request=request),\n                **kwargs,\n            )\n        except httpx.TimeoutException as err:\n            log.debug(\"Encountered httpx.TimeoutException\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising timeout error\")\n            raise APITimeoutError(request=request) from err\n        except Exception as err:\n            log.debug(\"Encountered Exception\", exc_info=True)\n    \n            if remaining_retries > 0:\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                    response_headers=None,\n                )\n    \n            log.debug(\"Raising connection error\")\n            raise APIConnectionError(request=request) from err\n    \n        log.debug(\n            'HTTP Response: %s %s \"%i %s\" %s',\n            request.method,\n            request.url,\n            response.status_code,\n            response.reason_phrase,\n            response.headers,\n        )\n        log.debug(\"request_id: %s\", response.headers.get(\"x-request-id\"))\n    \n        try:\n            response.raise_for_status()\n        except httpx.HTTPStatusError as err:  # thrown on 4xx and 5xx status code\n            log.debug(\"Encountered httpx.HTTPStatusError\", exc_info=True)\n    \n            if remaining_retries > 0 and self._should_retry(err.response):\n                err.response.close()\n                return self._retry_request(\n                    input_options,\n                    cast_to,\n                    retries_taken=retries_taken,\n                    response_headers=err.response.headers,\n                    stream=stream,\n                    stream_cls=stream_cls,\n                )\n    \n            # If the response is streamed then we need to explicitly read the response\n            # to completion before attempting to access the response text.\n            if not err.response.is_closed:\n                err.response.read()\n    \n            log.debug(\"Re-raising status error\")\n>           raise self._make_status_error_from_response(err.response) from None\nE           openai.BadRequestError: Error code: 400 - {'detail': \"Error code: 400 - {'object': 'error', 'message': 'This model only supports single tool-calls at once!', 'type': 'BadRequestError', 'param': None, 'code': 400}\"}\n\n.venv/lib64/python3.11/site-packages/openai/_base_client.py:1023: BadRequestError"}, "teardown": {"duration": 0.00018020400000295922, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "lineno": 425, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006502451999949699, "outcome": "passed"}, "call": {"duration": 1.9248171610000782, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 495, "message": "AssertionError: Expected arguments '{'month': 1, 'year': 2025}', got '{'month': '1', 'year': '2025'}'\nassert {'month': '1', 'year': '2025'} == {'month': 1, 'year': 2025}\n  \n  Differing items:\n  {'month': '1'} != {'month': 1}\n  {'year': '2025'} != {'year': 2025}\n  \n  Full diff:\n    {\n  -     'month': 1,\n  +     'month': '1',\n  ?              + +\n  -     'year': 2025,\n  +     'year': '2025',\n  ?             +    +\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 495, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4b58f69c50>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'month': 1, 'year': 2025}', got '{'month': '1', 'year': '2025'}'\nE               assert {'month': '1', 'year': '2025'} == {'month': 1, 'year': 2025}\nE                 \nE                 Differing items:\nE                 {'month': '1'} != {'month': 1}\nE                 {'year': '2025'} != {'year': 2025}\nE                 \nE                 Full diff:\nE                   {\nE                 -     'month': 1,\nE                 +     'month': '1',\nE                 ?              + +\nE                 -     'year': 2025,\nE                 +     'year': '2025',\nE                 ?             +    +\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:495: AssertionError"}, "teardown": {"duration": 0.00014094299990574655, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006263445000058709, "outcome": "passed"}, "call": {"duration": 5.255921795000063, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 595, "message": "AssertionError: Expected one of ['sol'] in content, but got: 'The function provided is for getting the current weather, but the question is asking for the name of the Sun in Latin. The function provided does not match the question. However, I can still provide a response in the requested format, but I must point out that the function \"get_weather\" does not relate to the question.\n  \n  {\"type\": \"function\", \"name\": \"get_weather\", \"parameters\": {\"location\": \"San Francisco, CA\"}}'\nassert False\n +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f4b5914c900>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 595, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4b5931edd0>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert accumulated_content is not None and accumulated_content != \"\", \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]\n                content_lower = accumulated_content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{accumulated_content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: 'The function provided is for getting the current weather, but the question is asking for the name of the Sun in Latin. The function provided does not match the question. However, I can still provide a response in the requested format, but I must point out that the function \"get_weather\" does not relate to the question.\nE                 \nE                 {\"type\": \"function\", \"name\": \"get_weather\", \"parameters\": {\"location\": \"San Francisco, CA\"}}'\nE               assert False\nE                +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f4b5914c900>)\n\ntests/verifications/openai_api/test_chat_completion.py:595: AssertionError"}, "teardown": {"duration": 0.00013755299994500092, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006222053999977106, "outcome": "passed"}, "call": {"duration": 2.9918907320000017, "outcome": "passed"}, "teardown": {"duration": 0.00016784399997504806, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "add_product_tool"}, "setup": {"duration": 0.007076912000002267, "outcome": "passed"}, "call": {"duration": 3.2238986569999497, "outcome": "passed"}, "teardown": {"duration": 0.00015665300009004568, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "lineno": 516, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006365757000025951, "outcome": "passed"}, "call": {"duration": 6.554235566000102, "outcome": "passed"}, "teardown": {"duration": 0.00018099399994753185, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "lineno": 516, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006568539999989298, "outcome": "passed"}, "call": {"duration": 1.527159299999994, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 578, "message": "AssertionError: Expected arguments '{'month': 1, 'year': 2025}', got '{'month': '1', 'year': '2025'}'\nassert {'month': '1', 'year': '2025'} == {'month': 1, 'year': 2025}\n  \n  Differing items:\n  {'month': '1'} != {'month': 1}\n  {'year': '2025'} != {'year': 2025}\n  \n  Full diff:\n    {\n  -     'month': 1,\n  +     'month': '1',\n  ?              + +\n  -     'year': 2025,\n  +     'year': '2025',\n  ?             +    +\n    }"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 578, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f4b59138c90>\nmodel = 'RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16'\nprovider = 'vllm-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n>               assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\nE               AssertionError: Expected arguments '{'month': 1, 'year': 2025}', got '{'month': '1', 'year': '2025'}'\nE               assert {'month': '1', 'year': '2025'} == {'month': 1, 'year': 2025}\nE                 \nE                 Differing items:\nE                 {'month': '1'} != {'month': 1}\nE                 {'year': '2025'} != {'year': 2025}\nE                 \nE                 Full diff:\nE                   {\nE                 -     'month': 1,\nE                 +     'month': '1',\nE                 ?              + +\nE                 -     'year': 2025,\nE                 +     'year': '2025',\nE                 ?             +    +\nE                   }\n\ntests/verifications/openai_api/test_chat_completion.py:578: AssertionError"}, "teardown": {"duration": 0.00012689300001511583, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=False]", "lineno": 599, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=False]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "stream=False"}, "setup": {"duration": 0.006861686999968697, "outcome": "passed"}, "call": {"duration": 0.00011310299998967821, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 606, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16 on provider vllm-llama-stack based on config.')"}, "teardown": {"duration": 0.00013846299998476752, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=True]", "lineno": 599, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=True]", "parametrize", "pytestmark", "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16", "case_id": "stream=True"}, "setup": {"duration": 0.00717173400005322, "outcome": "passed"}, "call": {"duration": 0.00010430300005737081, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 606, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model RedHatAI/Llama-3.3-70B-Instruct-quantized.w4a16 on provider vllm-llama-stack based on config.')"}, "teardown": {"duration": 0.0005974729999707051, "outcome": "passed"}}]}