{"created": 1746704193.6468556, "duration": 103.77786135673523, "exitcode": 1, "root": "/actions-runner/_work/lls-openai-client/lls-openai-client", "environment": {}, "summary": {"passed": 30, "skipped": 4, "failed": 4, "total": 38, "collected": 38}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "type": "Module"}]}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py", "outcome": "passed", "result": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "type": "Function", "lineno": 46}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "type": "Function", "lineno": 65}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "type": "Function", "lineno": 89}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "type": "Function", "lineno": 110}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 133}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 152}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "type": "Function", "lineno": 176}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "type": "Function", "lineno": 199}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 221}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 245}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 273}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 297}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 324}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "type": "Function", "lineno": 347}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "type": "Function", "lineno": 375}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "type": "Function", "lineno": 466}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "type": "Function", "lineno": 549}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "type": "Function", "lineno": 549}]}], "tests": [{"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "earth"}, "setup": {"duration": 0.03408576499998617, "outcome": "passed"}, "call": {"duration": 0.5987031730000467, "outcome": "passed"}, "teardown": {"duration": 0.00019202499998982603, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "lineno": 46, "outcome": "passed", "keywords": ["test_chat_non_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "saturn"}, "setup": {"duration": 0.007252797999967697, "outcome": "passed"}, "call": {"duration": 2.2436574089999795, "outcome": "passed"}, "teardown": {"duration": 0.0001581829999963702, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-earth]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-earth", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "earth"}, "setup": {"duration": 0.007179887000006602, "outcome": "passed"}, "call": {"duration": 0.5924452070000257, "outcome": "passed"}, "teardown": {"duration": 0.00016480400000773443, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "lineno": 65, "outcome": "passed", "keywords": ["test_chat_streaming_basic[llama3.3:70b-instruct-q4_K_M-saturn]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-saturn", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "saturn"}, "setup": {"duration": 0.007049333999987084, "outcome": "passed"}, "call": {"duration": 4.13512347599999, "outcome": "passed"}, "teardown": {"duration": 0.0001788940000437833, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_missing"}, "setup": {"duration": 0.0066855059999966215, "outcome": "passed"}, "call": {"duration": 0.004954799000017829, "outcome": "passed"}, "teardown": {"duration": 0.0002540849999945749, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006260627000017394, "outcome": "passed"}, "call": {"duration": 0.004204651999998532, "outcome": "passed"}, "teardown": {"duration": 0.00013586299996859452, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006211085000018102, "outcome": "passed"}, "call": {"duration": 0.1339469189999818, "outcome": "passed"}, "teardown": {"duration": 0.00016273399995725413, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.00679552799999783, "outcome": "passed"}, "call": {"duration": 0.07279175200000054, "outcome": "passed"}, "teardown": {"duration": 0.00011093299997355643, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "lineno": 89, "outcome": "passed", "keywords": ["test_chat_non_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.006616925000002993, "outcome": "passed"}, "call": {"duration": 0.0940443769999888, "outcome": "passed"}, "teardown": {"duration": 0.00013224299999592404, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_missing]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_missing", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_missing"}, "setup": {"duration": 0.006791217999989385, "outcome": "passed"}, "call": {"duration": 0.003297452000026624, "outcome": "passed"}, "teardown": {"duration": 0.0001266030000124374, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-messages_role_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-messages_role_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "messages_role_invalid"}, "setup": {"duration": 0.006630604999998013, "outcome": "passed"}, "call": {"duration": 0.0037756219999778295, "outcome": "passed"}, "teardown": {"duration": 0.0001049720000310117, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_invalid"}, "setup": {"duration": 0.006581154000002698, "outcome": "passed"}, "call": {"duration": 0.0776119079999944, "outcome": "passed"}, "teardown": {"duration": 0.00012770299997555412, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tool_choice_no_tools", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tool_choice_no_tools"}, "setup": {"duration": 0.006787568999982341, "outcome": "passed"}, "call": {"duration": 0.05917462400003615, "outcome": "passed"}, "teardown": {"duration": 0.0001588540000057037, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "lineno": 110, "outcome": "passed", "keywords": ["test_chat_streaming_error_handling[llama3.3:70b-instruct-q4_K_M-tools_type_invalid]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-tools_type_invalid", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "tools_type_invalid"}, "setup": {"duration": 0.006781908000050407, "outcome": "passed"}, "call": {"duration": 0.07832565299997896, "outcome": "passed"}, "teardown": {"duration": 0.0001559729999485171, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 133, "outcome": "skipped", "keywords": ["test_chat_non_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.007021792999978516, "outcome": "passed"}, "call": {"duration": 0.00011154200001328718, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 142, 'Skipped: Skipping test_chat_non_streaming_image for model llama3.3:70b-instruct-q4_K_M on provider ollama-llama-stack based on config.')"}, "teardown": {"duration": 0.00011697200000071462, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 152, "outcome": "skipped", "keywords": ["test_chat_streaming_image[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.012890642000002117, "outcome": "passed"}, "call": {"duration": 0.00010860300000103962, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 161, 'Skipped: Skipping test_chat_streaming_image for model llama3.3:70b-instruct-q4_K_M on provider ollama-llama-stack based on config.')"}, "teardown": {"duration": 0.00012321299999484836, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "calendar"}, "setup": {"duration": 0.00605295300005082, "outcome": "passed"}, "call": {"duration": 1.9020130879999897, "outcome": "passed"}, "teardown": {"duration": 0.0001463639999883526, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "lineno": 176, "outcome": "passed", "keywords": ["test_chat_non_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "math"}, "setup": {"duration": 0.006627535000006901, "outcome": "passed"}, "call": {"duration": 9.94108817099999, "outcome": "passed"}, "teardown": {"duration": 0.00016681399995377433, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-calendar]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-calendar", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "calendar"}, "setup": {"duration": 0.006680386999960319, "outcome": "passed"}, "call": {"duration": 1.9801898260000144, "outcome": "passed"}, "teardown": {"duration": 0.00014019299999290524, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "lineno": 199, "outcome": "passed", "keywords": ["test_chat_streaming_structured_output[llama3.3:70b-instruct-q4_K_M-math]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-math", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "math"}, "setup": {"duration": 0.006418420000045444, "outcome": "passed"}, "call": {"duration": 5.473216112999921, "outcome": "passed"}, "teardown": {"duration": 0.000162482999940039, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 221, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006310348000056365, "outcome": "passed"}, "call": {"duration": 1.485995457999934, "outcome": "passed"}, "teardown": {"duration": 0.00015935299995817331, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 245, "outcome": "passed", "keywords": ["test_chat_streaming_tool_calling[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006293897999967157, "outcome": "passed"}, "call": {"duration": 1.3518319940000083, "outcome": "passed"}, "teardown": {"duration": 0.0001483739999912359, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 273, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.0062762769999835655, "outcome": "passed"}, "call": {"duration": 1.3428747980000253, "outcome": "passed"}, "teardown": {"duration": 0.00013964300001134689, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 297, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_required[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006322727999986455, "outcome": "passed"}, "call": {"duration": 1.362451376000081, "outcome": "passed"}, "teardown": {"duration": 0.0001826540000138266, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 324, "outcome": "passed", "keywords": ["test_chat_non_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006254097000010006, "outcome": "passed"}, "call": {"duration": 18.305867050000074, "outcome": "passed"}, "teardown": {"duration": 0.00014849299998331844, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "lineno": 347, "outcome": "passed", "keywords": ["test_chat_streaming_tool_choice_none[llama3.3:70b-instruct-q4_K_M-case0]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-case0", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "case0"}, "setup": {"duration": 0.006518163000009736, "outcome": "passed"}, "call": {"duration": 13.105793840999922, "outcome": "passed"}, "teardown": {"duration": 0.00018614400005390053, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.015583242000047903, "outcome": "passed"}, "call": {"duration": 1.4884690569999748, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError: Expected 0 tool calls, but got 1\nassert 1 == 0\n +  where 1 = len(([ChatCompletionMessageToolCall(id='call_32qnr314', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)]))\n +    where [ChatCompletionMessageToolCall(id='call_32qnr314', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_32qnr314', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)]).tool_calls"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 434, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f0ae3168b10>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n>           assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\nE           AssertionError: Expected 0 tool calls, but got 1\nE           assert 1 == 0\nE            +  where 1 = len(([ChatCompletionMessageToolCall(id='call_32qnr314', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)]))\nE            +    where [ChatCompletionMessageToolCall(id='call_32qnr314', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)] = ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_32qnr314', function=Function(arguments='{\"location\":\"Rome, Italy\"}', name='get_weather'), type='function', index=0)]).tool_calls\n\ntests/verifications/openai_api/test_chat_completion.py:434: AssertionError"}, "teardown": {"duration": 0.00016841399997247208, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.006559293999998772, "outcome": "passed"}, "call": {"duration": 2.4071596979999867, "outcome": "passed"}, "teardown": {"duration": 0.0001477429999567903, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "add_product_tool"}, "setup": {"duration": 0.0064937419999751, "outcome": "passed"}, "call": {"duration": 3.9900342040000396, "outcome": "passed"}, "teardown": {"duration": 0.00013978300000871968, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "lineno": 375, "outcome": "failed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006607415000075889, "outcome": "passed"}, "call": {"duration": 2.518657341999983, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 462, "message": "AssertionError: Expected one of ['no', 'no events found', 'no meetings'] in content, but got: 'Yes'\nassert False\n +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f0ae32a89e0>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 462, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f0ae33005d0>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'get_then_create_event_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'date': '2025-03-03', ...ents found for 2025-03-03 at 10:00'}\"}, {'response': \"{'response': 'Successfully created new event with id: e_123'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_non_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\"\n        Test cases for multi-turn tool calling.\n        Tool calls are asserted.\n        Tool responses are provided in the test case.\n        Final response is asserted.\n        \"\"\"\n    \n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        # Create a copy of the messages list to avoid modifying the original\n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        # Use deepcopy to prevent modification across runs/parametrization\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        # keep going until either\n        # 1. we have messages to test in multi-turn\n        # 2. no messages but last message is tool response\n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            # do not take new messages if last message is tool response\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                # Ensure new_messages is a list of message objects\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    # If it's a single message object, add it directly\n                    messages.append(new_messages)\n    \n            # --- API Call ---\n            response = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=False,\n            )\n    \n            # --- Process Response ---\n            assistant_message = response.choices[0].message\n            messages.append(assistant_message.model_dump(exclude_unset=True))\n    \n            assert assistant_message.role == \"assistant\"\n    \n            # Get the expected result data\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            # --- Assertions based on expected result ---\n            assert len(assistant_message.tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(assistant_message.tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                tool_call = assistant_message.tool_calls[0]\n                assert tool_call.function.name == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call.function.name}'\"\n                )\n                # Parse the JSON string arguments before comparing\n                actual_arguments = json.loads(tool_call.function.arguments)\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert assistant_message.content is not None, \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]  # This is now a list\n                content_lower = assistant_message.content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{assistant_message.content}'\"\n                )\nE               AssertionError: Expected one of ['no', 'no events found', 'no meetings'] in content, but got: 'Yes'\nE               assert False\nE                +  where False = any(<generator object test_chat_non_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f0ae32a89e0>)\n\ntests/verifications/openai_api/test_chat_completion.py:462: AssertionError"}, "teardown": {"duration": 0.00015985300001375435, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "lineno": 375, "outcome": "passed", "keywords": ["test_chat_non_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006572784000013598, "outcome": "passed"}, "call": {"duration": 4.912762296999972, "outcome": "passed"}, "teardown": {"duration": 0.00015242300003137643, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-text_then_weather_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "text_then_weather_tool"}, "setup": {"duration": 0.006626435000043784, "outcome": "passed"}, "call": {"duration": 0.9715602479999461, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 545, "message": "AssertionError: Expected one of ['sol'] in content, but got: 'This task is beyond my capabilities with the given functions.'\nassert False\n +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f0ae32a89e0>)"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 545, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-text_then_weather_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f0ae3327090>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'text_then_weather_tool', 'expected': [{'answer': ['sol'], 'num_tool_calls': 0}, {'num_tool_calls': 1, 'to...], 'type': 'object'}}, 'type': 'function'}]}, 'tool_responses': [{'response': \"{'response': '70 degrees and foggy'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n                assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\n                # Parse the accumulated arguments string for comparison\n                actual_arguments = json.loads(tool_call[\"function\"][\"arguments\"])\n                assert actual_arguments == expected[\"tool_arguments\"], (\n                    f\"Expected arguments '{expected['tool_arguments']}', got '{actual_arguments}'\"\n                )\n    \n                # Prepare and append the tool response for the next turn\n                tool_response = tool_responses.pop(0)\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call[\"id\"],\n                        \"content\": tool_response[\"response\"],\n                    }\n                )\n            else:\n                assert accumulated_content is not None and accumulated_content != \"\", \"Expected content, but none received.\"\n                expected_answers = expected[\"answer\"]\n                content_lower = accumulated_content.lower()\n>               assert any(ans.lower() in content_lower for ans in expected_answers), (\n                    f\"Expected one of {expected_answers} in content, but got: '{accumulated_content}'\"\n                )\nE               AssertionError: Expected one of ['sol'] in content, but got: 'This task is beyond my capabilities with the given functions.'\nE               assert False\nE                +  where False = any(<generator object test_chat_streaming_multi_turn_tool_calling.<locals>.<genexpr> at 0x7f0ae32a89e0>)\n\ntests/verifications/openai_api/test_chat_completion.py:545: AssertionError"}, "teardown": {"duration": 0.00013492300001871627, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-weather_tool_then_text]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-weather_tool_then_text", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "weather_tool_then_text"}, "setup": {"duration": 0.0062040359999855355, "outcome": "passed"}, "call": {"duration": 2.428923656000052, "outcome": "passed"}, "teardown": {"duration": 0.00016548299993246474, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-add_product_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-add_product_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "add_product_tool"}, "setup": {"duration": 0.006775148000087938, "outcome": "passed"}, "call": {"duration": 6.409851774000003, "outcome": "passed"}, "teardown": {"duration": 0.00017171399997550907, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "lineno": 466, "outcome": "passed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-get_then_create_event_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "get_then_create_event_tool"}, "setup": {"duration": 0.006424781000077928, "outcome": "passed"}, "call": {"duration": 7.817518629000006, "outcome": "passed"}, "teardown": {"duration": 0.0003183670000908023, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "lineno": 466, "outcome": "failed", "keywords": ["test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "compare_monthly_expense_tool"}, "setup": {"duration": 0.006327668999915659, "outcome": "passed"}, "call": {"duration": 5.852822340999978, "outcome": "failed", "crash": {"path": "/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'compareMonthlyExpenses'\nassert 'compareMonthlyExpenses' == 'getMonthlyExpenseSummary'\n  \n  - getMonthlyExpenseSummary\n  + compareMonthlyExpenses"}, "traceback": [{"path": "tests/verifications/openai_api/test_chat_completion.py", "lineno": 523, "message": "AssertionError"}], "longrepr": "request = <FixtureRequest for <Function test_chat_streaming_multi_turn_tool_calling[llama3.3:70b-instruct-q4_K_M-compare_monthly_expense_tool]>>\nopenai_client = <openai.OpenAI object at 0x7f0ae3319490>\nmodel = 'llama3.3:70b-instruct-q4_K_M', provider = 'ollama-llama-stack'\nverification_config = {'providers': {'cerebras': {'api_key_var': 'CEREBRAS_API_KEY', 'base_url': 'https://api.cerebras.ai/v1', 'model_displa...-versatile', 'meta-llama/llama-4-scout-17b-16e-instruct', 'meta-llama/llama-4-maverick-17b-128e-instruct'], ...}, ...}}\ncase = {'case_id': 'compare_monthly_expense_tool', 'expected': [{'num_tool_calls': 1, 'tool_arguments': {'month': 1, 'year': ... 'Total expenses for January 2025: $1000'}\"}, {'response': \"{'response': 'Total expenses for February 2024: $2000'}\"}]}\n\n    @pytest.mark.parametrize(\n        \"case\",\n        chat_completion_test_cases.get(\"test_chat_multi_turn_tool_calling\", {}).get(\"test_params\", {}).get(\"case\", []),\n        ids=case_id_generator,\n    )\n    def test_chat_streaming_multi_turn_tool_calling(request, openai_client, model, provider, verification_config, case):\n        \"\"\" \"\"\"\n        test_name_base = get_base_test_name(request)\n        if should_skip_test(verification_config, provider, model, test_name_base):\n            pytest.skip(f\"Skipping {test_name_base} for model {model} on provider {provider} based on config.\")\n    \n        messages = []\n        tools = case[\"input\"][\"tools\"]\n        expected_results = copy.deepcopy(case[\"expected\"])\n        tool_responses = copy.deepcopy(case.get(\"tool_responses\", []))\n        input_messages_turns = copy.deepcopy(case[\"input\"][\"messages\"])\n    \n        while len(input_messages_turns) > 0 or (len(messages) > 0 and messages[-1][\"role\"] == \"tool\"):\n            if len(messages) == 0 or messages[-1][\"role\"] != \"tool\":\n                new_messages = input_messages_turns.pop(0)\n                if isinstance(new_messages, list):\n                    messages.extend(new_messages)\n                else:\n                    messages.append(new_messages)\n    \n            # --- API Call (Streaming) ---\n            stream = openai_client.chat.completions.create(\n                model=model,\n                messages=messages,\n                tools=tools,\n                stream=True,\n            )\n    \n            # --- Process Stream ---\n            accumulated_content, accumulated_tool_calls = _accumulate_streaming_tool_calls(stream)\n    \n            # --- Construct Assistant Message for History ---\n            assistant_message_dict = {\"role\": \"assistant\"}\n            if accumulated_content:\n                assistant_message_dict[\"content\"] = accumulated_content\n            if accumulated_tool_calls:\n                assistant_message_dict[\"tool_calls\"] = accumulated_tool_calls\n    \n            messages.append(assistant_message_dict)\n    \n            # --- Assertions ---\n            expected = expected_results.pop(0)\n            num_tool_calls = expected[\"num_tool_calls\"]\n    \n            assert len(accumulated_tool_calls or []) == num_tool_calls, (\n                f\"Expected {num_tool_calls} tool calls, but got {len(accumulated_tool_calls or [])}\"\n            )\n    \n            if num_tool_calls > 0:\n                # Use the first accumulated tool call for assertion\n                tool_call = accumulated_tool_calls[0]\n>               assert tool_call[\"function\"][\"name\"] == expected[\"tool_name\"], (\n                    f\"Expected tool '{expected['tool_name']}', got '{tool_call['function']['name']}'\"\n                )\nE               AssertionError: Expected tool 'getMonthlyExpenseSummary', got 'compareMonthlyExpenses'\nE               assert 'compareMonthlyExpenses' == 'getMonthlyExpenseSummary'\nE                 \nE                 - getMonthlyExpenseSummary\nE                 + compareMonthlyExpenses\n\ntests/verifications/openai_api/test_chat_completion.py:523: AssertionError"}, "teardown": {"duration": 0.0001436229999853822, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=False]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-stream=False", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "stream=False"}, "setup": {"duration": 0.0072182989999873826, "outcome": "passed"}, "call": {"duration": 0.00011228299990762025, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama3.3:70b-instruct-q4_K_M on provider ollama-llama-stack based on config.')"}, "teardown": {"duration": 0.00010816200006047438, "outcome": "passed"}}, {"nodeid": "tests/verifications/openai_api/test_chat_completion.py::test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "lineno": 549, "outcome": "skipped", "keywords": ["test_chat_multi_turn_multiple_images[llama3.3:70b-instruct-q4_K_M-stream=True]", "parametrize", "pytestmark", "llama3.3:70b-instruct-q4_K_M-stream=True", "test_chat_completion.py", "openai_api", "verifications", "tests", "lls-openai-client", ""], "metadata": {"model": "llama3.3:70b-instruct-q4_K_M", "case_id": "stream=True"}, "setup": {"duration": 0.007083435000026839, "outcome": "passed"}, "call": {"duration": 9.674200009612832e-05, "outcome": "skipped", "longrepr": "('/actions-runner/_work/lls-openai-client/lls-openai-client/tests/verifications/openai_api/test_chat_completion.py', 556, 'Skipped: Skipping test_chat_multi_turn_multiple_images for model llama3.3:70b-instruct-q4_K_M on provider ollama-llama-stack based on config.')"}, "teardown": {"duration": 0.0006332240000119782, "outcome": "passed"}}]}